{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"0_BeeChain_0.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNNaajf4kScAIseIcD78tcn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"GjYh7u4jsWul"},"source":["Created by Joan-Marc Fisa\n","\n","- Numerai: [FisaGol](https://numer.ai/fisagol)\n","\n","- Twitter: [@fisagol](https://twitter.com/fisagol)\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YcqWFVQgKxXu","executionInfo":{"status":"ok","timestamp":1630849324352,"user_tz":-120,"elapsed":14282,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"7bcd8c0c-380a-4842-8652-a7a5845e405b"},"source":["from google.colab import drive\n","drive.mount('drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at drive\n"]}]},{"cell_type":"code","metadata":{"id":"b_mi2WLqDyYT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mKvhNddJw99J","executionInfo":{"status":"ok","timestamp":1631218498149,"user_tz":-120,"elapsed":3612,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"abc706be-6b66-4827-d3f2-266f7a974a68"},"source":["pip install numerapi"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numerapi in /usr/local/lib/python3.7/dist-packages (2.8.1)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.23.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (7.1.2)\n","Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from numerapi) (4.62.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from numerapi) (2018.9)\n","Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (1.1.5)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->numerapi) (1.19.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->numerapi) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (3.0.4)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hC_kTcjoxBl6","executionInfo":{"status":"ok","timestamp":1631218500651,"user_tz":-120,"elapsed":2504,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"7c49b6b0-2e2d-4dcd-becf-23887c2fc86f"},"source":["pip install halo"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: halo in /usr/local/lib/python3.7/dist-packages (0.0.31)\n","Requirement already satisfied: colorama>=0.3.9 in /usr/local/lib/python3.7/dist-packages (from halo) (0.4.4)\n","Requirement already satisfied: spinners>=0.0.24 in /usr/local/lib/python3.7/dist-packages (from halo) (0.0.24)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from halo) (1.15.0)\n","Requirement already satisfied: log-symbols>=0.0.14 in /usr/local/lib/python3.7/dist-packages (from halo) (0.0.14)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from halo) (1.1.0)\n"]}]},{"cell_type":"code","metadata":{"id":"eURkk6k8C7yA","executionInfo":{"status":"ok","timestamp":1631218501754,"user_tz":-120,"elapsed":1105,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}}},"source":["import os\n","import requests\n","import numpy as np\n","import pandas as pd\n","import scipy\n","from halo import Halo\n","from pathlib import Path\n","import json\n","from scipy.stats import skew, kurtosis\n","from lightgbm import LGBMRegressor\n","import gc\n","from numerapi import NumerAPI"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"V69uAJPLDJSP","executionInfo":{"status":"ok","timestamp":1631218501754,"user_tz":-120,"elapsed":2,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}}},"source":["ERA_COL = \"era\"\n","TARGET_COL = \"target\"\n","spinner = Halo(text='', spinner='dots')\n","\n","MODEL_FOLDER = \"models\"\n","MODEL_CONFIGS_FOLDER = \"model_configs\"\n","PREDICTION_FILES_FOLDER = \"prediction_files\""],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"bAmk5_EmEPoV","executionInfo":{"status":"ok","timestamp":1631218501755,"user_tz":-120,"elapsed":3,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}}},"source":["EXAMPLE_PREDS_COL = \"example_preds\"\n","\n","model_params = {\"n_estimators\": 2000,\n","                \"learning_rate\": 0.01,\n","                \"max_depth\": 5,\n","                \"num_leaves\": 2 ** 5,\n","                \"colsample_bytree\": 0.1}\n","\n","downsample_cross_val = 20\n","downsample_full_train = 1"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"OLnjWRTtDR4_","executionInfo":{"status":"ok","timestamp":1631216500685,"user_tz":-120,"elapsed":16,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}}},"source":["#####################    UTILITI FUNCTIONS   ##################################"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"IrNbIGarC70M","executionInfo":{"status":"ok","timestamp":1631218504579,"user_tz":-120,"elapsed":1231,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}}},"source":["def save_model(model, name):\n","    try:\n","        Path(MODEL_FOLDER).mkdir(exist_ok=True, parents=True)\n","    except Exception as ex:\n","        pass\n","    pd.to_pickle(model, f\"{MODEL_FOLDER}/{name}.pkl\")\n","\n","\n","def load_model(name):\n","    path = Path(f\"{MODEL_FOLDER}/{name}.pkl\")\n","    if path.is_file():\n","        model = pd.read_pickle(f\"{MODEL_FOLDER}/{name}.pkl\")\n","    else:\n","        model = False\n","    return model\n","\n","\n","def save_model_config(model_config, model_name):\n","    try:\n","        Path(MODEL_CONFIGS_FOLDER).mkdir(exist_ok=True, parents=True)\n","    except Exception as ex:\n","        pass\n","    with open(f\"{MODEL_CONFIGS_FOLDER}/{model_name}.json\", 'w') as fp:\n","        json.dump(model_config, fp)\n","\n","\n","def load_model_config(model_name):\n","    path_str = f\"{MODEL_CONFIGS_FOLDER}/{model_name}.json\"\n","    path = Path(path_str)\n","    if path.is_file():\n","        with open(path_str, 'r') as fp:\n","            model_config = json.load(fp)\n","    else:\n","        model_config = False\n","    return model_config\n","\n","\n","def get_biggest_change_features(corrs, n):\n","    all_eras = corrs.index.sort_values()\n","    h1_eras = all_eras[:len(all_eras) // 2]\n","    h2_eras = all_eras[len(all_eras) // 2:]\n","\n","    h1_corr_means = corrs.loc[h1_eras, :].mean()\n","    h2_corr_means = corrs.loc[h2_eras, :].mean()\n","\n","    corr_diffs = h2_corr_means - h1_corr_means\n","    worst_n = corr_diffs.abs().sort_values(ascending=False).head(n).index.tolist()\n","    return worst_n\n","\n","\n","def get_time_series_cross_val_splits(data, cv, embargo):\n","    all_train_eras = data[ERA_COL].unique()\n","    cv = 3\n","    len_split = len(all_train_eras) // 3\n","    test_splits = [all_train_eras[i * len_split:(i + 1) * len_split] for i in range(cv)]\n","    # fix the last test split to have all the last eras, in case the number of eras wasn't divisible by cv\n","    test_splits[-1] = np.append(test_splits[-1], all_train_eras[-1])\n","\n","    train_splits = []\n","    for test_split in test_splits:\n","        test_split_max = int(np.max(test_split))\n","        test_split_min = int(np.min(test_split))\n","        # get all of the eras that aren't in the test split\n","        train_split_not_embargoed = [e for e in all_train_eras if not (test_split_min <= int(e) <= test_split_max)]\n","        # embargo the train split so we have no leakage.\n","        # one era is length 5, so we need to embargo by target_length/5 eras.\n","        # To be consistent for all targets, let's embargo everything by 60/5 == 12 eras.\n","        train_split = [e for e in train_split_not_embargoed if\n","                       abs(int(e) - test_split_max) > 12 and abs(int(e) - test_split_min) > 12]\n","        train_splits.append(train_split)\n","\n","    # convenient way to iterate over train and test splits\n","    train_test_zip = zip(train_splits, test_splits)\n","    return train_test_zip\n","\n","\n","def neutralize(df, columns, neutralizers=None, proportion=1.0, normalize=True, era_col=\"era\"):\n","    if neutralizers is None:\n","        neutralizers = []\n","    unique_eras = df[era_col].unique()\n","    computed = []\n","    for u in unique_eras:\n","        df_era = df[df[era_col] == u]\n","        scores = df_era[columns].values\n","        if normalize:\n","            scores2 = []\n","            for x in scores.T:\n","                x = (scipy.stats.rankdata(x, method='ordinal') - .5) / len(x)\n","                x = scipy.stats.norm.ppf(x)\n","                scores2.append(x)\n","            scores = np.array(scores2).T\n","        exposures = df_era[neutralizers].values\n","\n","        scores -= proportion * exposures.dot(\n","            np.linalg.pinv(exposures.astype(np.float32)).dot(scores.astype(np.float32)))\n","\n","        scores /= scores.std(ddof=0)\n","\n","        computed.append(scores)\n","\n","    return pd.DataFrame(np.concatenate(computed),\n","                        columns=columns,\n","                        index=df.index)\n","\n","def neutralize_series(series, by, proportion=1.0):\n","    scores = series.values.reshape(-1, 1)\n","    exposures = by.values.reshape(-1, 1)\n","\n","    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n","    exposures = np.hstack(\n","        (exposures,\n","         np.array([np.mean(series)] * len(exposures)).reshape(-1, 1)))\n","\n","    correction = proportion * (exposures.dot(\n","        np.linalg.lstsq(exposures, scores, rcond=None)[0]))\n","    corrected_scores = scores - correction\n","    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n","    return neutralized\n","\n","\n","def unif(df):\n","    x = (df.rank(method=\"first\") - 0.5) / len(df)\n","    return pd.Series(x, index=df.index)\n","\n","\n","def get_feature_neutral_mean(df, prediction_col):\n","    feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n","    df.loc[:, \"neutral_sub\"] = neutralize(df, [prediction_col],\n","                                          feature_cols)[prediction_col]\n","    scores = df.groupby(\"era\").apply(\n","        lambda x: (unif(x[\"neutral_sub\"]).corr(x[TARGET_COL]))).mean()\n","    return np.mean(scores)\n","\n","def fast_score_by_date(df, columns, target, tb=None, era_col=\"era\"):\n","    unique_eras = df[era_col].unique()\n","    computed = []\n","    for u in unique_eras:\n","        df_era = df[df[era_col] == u]\n","        era_pred = np.float64(df_era[columns].values.T)\n","        era_target = np.float64(df_era[target].values.T)\n","\n","        if tb is None:\n","            ccs = np.corrcoef(era_target, era_pred)[0, 1:]\n","        else:\n","            tbidx = np.argsort(era_pred, axis=1)\n","            tbidx = np.concatenate([tbidx[:, :tb], tbidx[:, -tb:]], axis=1)\n","            ccs = [np.corrcoef(era_target[tmpidx], tmppred[tmpidx])[0, 1] for tmpidx, tmppred in zip(tbidx, era_pred)]\n","            ccs = np.array(ccs)\n","\n","        computed.append(ccs)\n","\n","    return pd.DataFrame(np.array(computed), columns=columns, index=df[era_col].unique())\n","\n","\n","def validation_metrics(validation_data, pred_cols, example_col, fast_mode=False):\n","    validation_stats = pd.DataFrame()\n","    feature_cols = [c for c in validation_data if c.startswith(\"feature_\")]\n","    for pred_col in pred_cols:\n","        # Check the per-era correlations on the validation set (out of sample)\n","        validation_correlations = validation_data.groupby(ERA_COL).apply(\n","            lambda d: unif(d[pred_col]).corr(d[TARGET_COL]))\n","\n","        mean = validation_correlations.mean()\n","        std = validation_correlations.std(ddof=0)\n","        sharpe = mean / std\n","\n","        validation_stats.loc[\"mean\", pred_col] = mean\n","        validation_stats.loc[\"std\", pred_col] = std\n","        validation_stats.loc[\"sharpe\", pred_col] = sharpe\n","\n","        rolling_max = (validation_correlations + 1).cumprod().rolling(window=9000,  # arbitrarily large\n","                                                                      min_periods=1).max()\n","        daily_value = (validation_correlations + 1).cumprod()\n","        max_drawdown = -((rolling_max - daily_value) / rolling_max).max()\n","        validation_stats.loc[\"max_drawdown\", pred_col] = max_drawdown\n","\n","        payout_scores = validation_correlations.clip(-0.25, 0.25)\n","        payout_daily_value = (payout_scores + 1).cumprod()\n","\n","        apy = (\n","            (\n","                (payout_daily_value.dropna().iloc[-1])\n","                ** (1 / len(payout_scores))\n","            )\n","            ** 49  # 52 weeks of compounding minus 3 for stake compounding lag\n","            - 1\n","        ) * 100\n","\n","        validation_stats.loc[\"apy\", pred_col] = apy\n","\n","        if not fast_mode:\n","            # Check the feature exposure of your validation predictions\n","            max_per_era = validation_data.groupby(ERA_COL).apply(\n","                lambda d: d[feature_cols].corrwith(d[pred_col]).abs().max())\n","            max_feature_exposure = max_per_era.mean()\n","            validation_stats.loc[\"max_feature_exposure\", pred_col] = max_feature_exposure\n","\n","            # Check feature neutral mean\n","            feature_neutral_mean = get_feature_neutral_mean(validation_data, pred_col)\n","            validation_stats.loc[\"feature_neutral_mean\", pred_col] = feature_neutral_mean\n","\n","            # Check top and bottom 200 metrics (TB200)\n","            tb200_validation_correlations = fast_score_by_date(\n","                validation_data,\n","                [pred_col],\n","                TARGET_COL,\n","                tb=200,\n","                era_col=ERA_COL\n","            )\n","\n","            tb200_mean = tb200_validation_correlations.mean()[pred_col]\n","            tb200_std = tb200_validation_correlations.std(ddof=0)[pred_col]\n","            tb200_sharpe = mean / std\n","\n","            validation_stats.loc[\"tb200_mean\", pred_col] = tb200_mean\n","            validation_stats.loc[\"tb200_std\", pred_col] = tb200_std\n","            validation_stats.loc[\"tb200_sharpe\", pred_col] = tb200_sharpe\n","\n","        # MMC over validation\n","        mmc_scores = []\n","        corr_scores = []\n","        for _, x in validation_data.groupby(ERA_COL):\n","            series = neutralize_series(unif(x[pred_col]), (x[example_col]))\n","            mmc_scores.append(np.cov(series, x[TARGET_COL])[0, 1] / (0.29 ** 2))\n","            corr_scores.append(unif(x[pred_col]).corr(x[TARGET_COL]))\n","\n","        val_mmc_mean = np.mean(mmc_scores)\n","        val_mmc_std = np.std(mmc_scores)\n","        corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n","        corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n","\n","        validation_stats.loc[\"mmc_mean\", pred_col] = val_mmc_mean\n","        validation_stats.loc[\"corr_plus_mmc_sharpe\", pred_col] = corr_plus_mmc_sharpe\n","\n","        # Check correlation with example predictions\n","        per_era_corrs = validation_data.groupby(ERA_COL).apply(lambda d: unif(d[pred_col]).corr(unif(d[example_col])))\n","        corr_with_example_preds = per_era_corrs.mean()\n","        validation_stats.loc[\"corr_with_example_preds\", pred_col] = corr_with_example_preds\n","\n","    # .transpose so that stats are columns and the model_name is the row\n","    return validation_stats.transpose()\n","\n","\n","\n","def download_file(url: str, dest_path: str, show_progress_bars: bool = True):\n","\n","    req = requests.get(url, stream=True)\n","    req.raise_for_status()\n","\n","    # Total size in bytes.\n","    total_size = int(req.headers.get('content-length', 0))\n","\n","    if os.path.exists(dest_path):\n","        file_size = os.stat(dest_path).st_size  # File size in bytes\n","        if file_size < total_size:\n","            # Download incomplete\n","            resume_header = {'Range': 'bytes=%d-' % file_size}\n","            req = requests.get(url, headers=resume_header, stream=True,\n","                               verify=False, allow_redirects=True)\n","        elif file_size == total_size:\n","            # Download complete\n","            return\n","        else:\n","            # Error, delete file and restart download\n","            os.remove(dest_path)\n","            file_size = 0\n","\n","    with open(dest_path, \"ab\") as dest_file:\n","        for chunk in req.iter_content(1024):\n","            dest_file.write(chunk)\n","\n","\n","def download_data(napi, filename, dest_path, round=None):\n","    spinner.start(f'Downloading {dest_path}')\n","    query = \"\"\"\n","            query ($filename: String!) {\n","                dataset(filename: $filename)\n","            }\n","            \"\"\"\n","    params = {\n","        'filename': filename\n","    }\n","    if round:\n","        query = \"\"\"\n","                    query ($filename: String!, $round: Int) {\n","                        dataset(filename: $filename, round: $round)\n","                    }\n","                    \"\"\"\n","        params['round'] = round\n","    dataset_url = napi.raw_query(query, params)['data']['dataset']\n","    download_file(dataset_url, dest_path, show_progress_bars=True)\n","    spinner.succeed()\n","    return dataset_url"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"KREDFdwbC731","executionInfo":{"status":"ok","timestamp":1631216501524,"user_tz":-120,"elapsed":3,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}}},"source":["#########################    EXEMPLE ADVANCED MODEL   ########################"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145},"id":"vOimw5LvMxc9","executionInfo":{"status":"ok","timestamp":1631218555687,"user_tz":-120,"elapsed":21992,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"ccdf6077-98ce-4c05-b927-d0610dc5f880"},"source":["napi = NumerAPI()\n","spinner = Halo(text='', spinner='dots')\n","\n","current_round = napi.get_current_round(tournament=8)  # tournament 8 is the primary Numerai Tournament\n","\n","# read in all of the new datas\n","# tournament data and example predictions change every week so we specify the round in their names\n","# training and validation data only change periodically, so no need to download them over again every single week\n","download_data(napi, 'numerai_training_data.parquet', 'numerai_training_data.parquet', round=current_round)\n","download_data(napi, 'numerai_tournament_data.parquet', f'numerai_tournament_data_{current_round}.parquet', round=current_round)\n","download_data(napi, 'numerai_validation_data.parquet', 'numerai_validation_data.parquet', round=current_round)\n","download_data(napi, 'example_predictions.parquet', f'example_predictions_{current_round}.parquet', round=current_round)\n","download_data(napi, 'example_validation_predictions.parquet', f'example_validation_predictions.parquet', round=current_round)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["✔ Downloading numerai_training_data.parquet\n","✔ Downloading numerai_tournament_data_280.parquet\n","✔ Downloading numerai_validation_data.parquet\n","✔ Downloading example_predictions_280.parquet\n","✔ Downloading example_validation_predictions.parquet\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'https://numerai-datasets.s3.amazonaws.com/280/v3/example_validation_predictions.parquet?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIJRTTNQLX2AQCKYA%2F20210909%2Fus-west-1%2Fs3%2Faws4_request&X-Amz-Date=20210909T201554Z&X-Amz-Expires=604799&X-Amz-SignedHeaders=host&X-Amz-Signature=d4bcc9344c916083e7f920d3a21811cdb79d0ed2c952aea1a0ed3b8a595223ea'"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"AlkDCM5oM59S","executionInfo":{"status":"ok","timestamp":1631218993259,"user_tz":-120,"elapsed":16663,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}}},"source":["training_data = pd.read_parquet('numerai_training_data.parquet')\n","tournament_data = pd.read_parquet(f'numerai_tournament_data_{current_round}.parquet')\n","validation_data = pd.read_parquet('numerai_validation_data.parquet')\n","example_preds = pd.read_parquet(f'example_predictions_{current_round}.parquet')\n","validation_preds = pd.read_parquet('example_validation_predictions.parquet')"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rX7poH9xC75y","executionInfo":{"status":"ok","timestamp":1631217293965,"user_tz":-120,"elapsed":792444,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"5419c4c8-fa71-4dae-beb3-0bff0dc66723"},"source":["model_config_name = \"advanced_example_model\"\n","\n","napi = NumerAPI()\n","\n","current_round = napi.get_current_round(tournament=8)  # tournament 8 is the primary Numerai Tournament\n","\n","print(\"Entering model selection loop.  This may take awhile.\")\n","\n","model_config = {}\n","print('downloading training_data')\n","download_data(napi, 'numerai_training_data.parquet', 'numerai_training_data.parquet', round=current_round)\n","\n","print(\"reading training data from local file\")\n","training_data = pd.read_parquet('numerai_training_data.parquet')\n","\n","# keep track of some prediction columns\n","ensemble_cols = set()\n","pred_cols = set()\n","\n","# pick some targets to use\n","possible_targets = [c for c in training_data.columns if c.startswith(\"target_\")]\n","# randomly pick a handful of targets\n","# this can be vastly improved\n","targets = [\"target\", \"target_nomi_60\", \"target_jerome_20\"]\n","\n","# all the possible features to train on\n","feature_cols = [c for c in training_data if c.startswith(\"feature_\")]\n","\n","\"\"\" do cross val to get out of sample training preds\"\"\"\n","cv = 3\n","train_test_zip = get_time_series_cross_val_splits(training_data, cv=cv, embargo=12)\n","# get out of sample training preds via embargoed time series cross validation\n","# optionally downsample training data to speed up this section.\n","print(\"entering time series cross validation loop\")\n","for split, train_test_split in enumerate(train_test_zip):\n","    gc.collect()\n","    print(f\"doing split {split+1} out of {cv}\")\n","    train_split, test_split = train_test_split\n","    train_split_index = training_data[ERA_COL].isin(train_split)\n","    test_split_index = training_data[ERA_COL].isin(test_split)\n","    downsampled_train_split_index = train_split_index[train_split_index].index[::downsample_cross_val]\n","\n","    # getting the per era correlation of each feature vs the primary target across the training split\n","    print(\"getting feature correlations over time and identifying riskiest features\")\n","    all_feature_corrs_split = training_data.loc[downsampled_train_split_index, :].groupby(ERA_COL).apply(\n","        lambda d: d[feature_cols].corrwith(d[TARGET_COL]))\n","    # find the riskiest features by comparing their correlation vs the target in half 1 and half 2 of training data\n","    # there are probably more clever ways to do this\n","    riskiest_features_split = get_biggest_change_features(all_feature_corrs_split, 50)\n","\n","    print(f\"entering model training loop for split {split+1}\")\n","    for target in targets:\n","        model_name = f\"model_{target}\"\n","        print(f\"model: {model_name}\")\n","\n","        # train a model on the training split (and save it for future use)\n","        split_model_name = f\"model_{target}_split{split+1}cv{cv}downsample{downsample_cross_val}\"\n","        split_model = load_model(split_model_name)\n","        if not split_model:\n","            print(f\"training model: {model_name}\")\n","            split_model = LGBMRegressor(**model_params)\n","            split_model.fit(training_data.loc[downsampled_train_split_index, feature_cols],\n","                            training_data.loc[downsampled_train_split_index,\n","                                              [target]])\n","            save_model(split_model, split_model_name)\n","        # now we can predict on the test part of the split\n","        model_expected_features = split_model.booster_.feature_name()\n","        if set(model_expected_features) != set(feature_cols):\n","            print(f\"New features are available! Might want to retrain model {split_model_name}.\")\n","        print(f\"predicting {model_name}\")\n","        training_data.loc[test_split_index, f\"preds_{model_name}\"] = \\\n","            split_model.predict(training_data.loc[test_split_index, model_expected_features])\n","\n","        # do neutralization\n","        print(\"doing neutralization to riskiest features\")\n","        training_data.loc[test_split_index, f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(\n","            df=training_data.loc[test_split_index, :],\n","            columns=[f\"preds_{model_name}\"],\n","            neutralizers=riskiest_features_split,\n","            proportion=1.0,\n","            normalize=True,\n","            era_col=ERA_COL)[f\"preds_{model_name}\"]\n","\n","        # remember that we made all of these different pred columns\n","        pred_cols.add(f\"preds_{model_name}\")\n","        pred_cols.add(f\"preds_{model_name}_neutral_riskiest_50\")\n","\n","    print(\"creating ensembles\")\n","    # do ensembles\n","    training_data[\"ensemble_neutral_riskiest_50\"] = sum(\n","        [training_data[pred_col] for pred_col in pred_cols if pred_col.endswith(\"neutral_riskiest_50\")]).rank(\n","        pct=True)\n","    training_data[\"ensemble_not_neutral\"] = sum(\n","        [training_data[pred_col] for pred_col in pred_cols if \"neutral\" not in pred_col]).rank(pct=True)\n","    training_data[\"ensemble_all\"] = sum([training_data[pred_col] for pred_col in pred_cols]).rank(pct=True)\n","\n","    ensemble_cols.add(\"ensemble_neutral_riskiest_50\")\n","    ensemble_cols.add(\"ensemble_not_neutral\")\n","    ensemble_cols.add(\"ensemble_all\")\n"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Entering model selection loop.  This may take awhile.\n","downloading training_data\n","✔ Downloading numerai_training_data.parquet\n","reading training data from local file\n","entering time series cross validation loop\n","doing split 1 out of 3\n","getting feature correlations over time and identifying riskiest features\n","entering model training loop for split 1\n","model: model_target\n","training model: model_target\n","predicting model_target\n","doing neutralization to riskiest features\n","model: model_target_nomi_60\n","training model: model_target_nomi_60\n","predicting model_target_nomi_60\n","doing neutralization to riskiest features\n","model: model_target_jerome_20\n","training model: model_target_jerome_20\n","predicting model_target_jerome_20\n","doing neutralization to riskiest features\n","creating ensembles\n","doing split 2 out of 3\n","getting feature correlations over time and identifying riskiest features\n","entering model training loop for split 2\n","model: model_target\n","training model: model_target\n","predicting model_target\n","doing neutralization to riskiest features\n","model: model_target_nomi_60\n","training model: model_target_nomi_60\n","predicting model_target_nomi_60\n","doing neutralization to riskiest features\n","model: model_target_jerome_20\n","training model: model_target_jerome_20\n","predicting model_target_jerome_20\n","doing neutralization to riskiest features\n","creating ensembles\n","doing split 3 out of 3\n","getting feature correlations over time and identifying riskiest features\n","entering model training loop for split 3\n","model: model_target\n","training model: model_target\n","predicting model_target\n","doing neutralization to riskiest features\n","model: model_target_nomi_60\n","training model: model_target_nomi_60\n","predicting model_target_nomi_60\n","doing neutralization to riskiest features\n","model: model_target_jerome_20\n","training model: model_target_jerome_20\n","predicting model_target_jerome_20\n","doing neutralization to riskiest features\n","creating ensembles\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r2c6bQHjb05o","executionInfo":{"status":"ok","timestamp":1631217496380,"user_tz":-120,"elapsed":165722,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"df704f0d-6738-4e60-904b-bf24f96d79fe"},"source":["\n","\"\"\" Now get some stats and pick our favorite model\"\"\"\n","print(\"gathering validation metrics for out of sample training results\")\n","all_model_cols = list(pred_cols) + list(ensemble_cols)\n","# use example_col preds_model_target as an estimates since no example preds provided for training\n","# fast_mode=True so that we skip some of the stats that are slower to calculate\n","training_stats = validation_metrics(training_data, all_model_cols, example_col=\"preds_model_target\",\n","                                    fast_mode=True)\n","print(training_stats[[\"mean\", \"sharpe\"]].sort_values(by=\"sharpe\", ascending=False).to_markdown())"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["gathering validation metrics for out of sample training results\n","|                                                  |        mean |    sharpe |\n","|:-------------------------------------------------|------------:|----------:|\n","| preds_model_target                               | 0.0391384   | 1.61962   |\n","| ensemble_not_neutral                             | 0.0391384   | 1.61962   |\n","| preds_model_target_neutral_riskiest_50           | 0.0334507   | 1.58361   |\n","| ensemble_all                                     | 0.0160642   | 0.967286  |\n","| ensemble_neutral_riskiest_50                     | 0.0157643   | 0.951114  |\n","| preds_model_target_jerome_20                     | 0.000443636 | 0.0273348 |\n","| preds_model_target_nomi_60                       | 0.000443636 | 0.0273348 |\n","| preds_model_target_nomi_60_neutral_riskiest_50   | 0.000316699 | 0.0197886 |\n","| preds_model_target_jerome_20_neutral_riskiest_50 | 0.000316699 | 0.0197886 |\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uKUV3GXpINmX","executionInfo":{"status":"ok","timestamp":1631217544376,"user_tz":-120,"elapsed":198,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"1cbeb9b5-0794-4ee6-f8d5-b6e8daefe563"},"source":["# pick the model that has the highest correlation sharpe\n","best_pred_col = training_stats.sort_values(by=\"sharpe\", ascending=False).head(1).index[0]\n","print(f\"selecting model {best_pred_col} as our highest sharpe model in validation\")"],"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["selecting model preds_model_target as our highest sharpe model in validation\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t4CczCsDIJ4D","executionInfo":{"status":"ok","timestamp":1631217734434,"user_tz":-120,"elapsed":185032,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"20590a9e-6e39-4226-e171-bb0b5962f66e"},"source":["\"\"\" Now do a full train\"\"\"\n","print(\"entering full training section\")\n","# getting the per era correlation of each feature vs the target across all of training data\n","print(\"getting feature correlations with target and identifying riskiest features\")\n","all_feature_corrs = training_data.groupby(ERA_COL).apply(\n","    lambda d: d[feature_cols].corrwith(d[TARGET_COL]))\n","# find the riskiest features by comparing their correlation vs the target in half 1 and half 2 of training data\n","riskiest_features = get_biggest_change_features(all_feature_corrs, 50)"],"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["entering full training section\n","getting feature correlations with target and identifying riskiest features\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lyauY543Ir0m","outputId":"bfbfb8cd-2212-41ea-8f0d-3042af38658d"},"source":["for target in targets:\n","    gc.collect()\n","    model_name = f\"model_{target}_downsample{downsample_full_train}\"\n","    model = load_model(model_name)\n","    if not model:\n","        print(f\"training {model_name}\")\n","        model = LGBMRegressor(**model_params)\n","        # train on all of train, predict on val, predict on tournament\n","        model.fit(training_data.iloc[::downsample_full_train].loc[:, feature_cols],\n","                  training_data.iloc[::downsample_full_train][target])\n","        save_model(model, model_name)\n","    gc.collect()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["training model_target_downsample1\n"]}]},{"cell_type":"code","metadata":{"id":"7cg7iNsUItdC"},"source":["model_config[\"feature_cols\"] = feature_cols\n","model_config[\"targets\"] = targets\n","model_config[\"best_pred_col\"] = best_pred_col\n","model_config[\"riskiest_features\"] = riskiest_features\n","print(f\"saving model config for {model_config_name}\")\n","save_model_config(model_config, model_config_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X7ynF-FjE3hF"},"source":["#######################################################################################\n","\n","\"\"\" Things that we always do even if we've already trained \"\"\"\n","gc.collect()\n","print(\"downloading tournament_data\")\n","download_data(napi, 'numerai_tournament_data.parquet', f'numerai_tournament_data_{current_round}.parquet',\n","              round=current_round)\n","print(\"downloading validation_data\")\n","download_data(napi, 'numerai_validation_data.parquet', 'numerai_validation_data.parquet', round=current_round)\n","print(\"downloading example_predictions\")\n","download_data(napi, 'example_predictions.parquet', f'example_predictions_{current_round}.parquet',\n","              round=current_round)\n","print(\"downloading example_validation_predictions\")\n","download_data(napi, 'example_validation_predictions.parquet', f'example_validation_predictions.parquet',\n","              round=current_round)\n","\n","print(\"reading tournament_data\")\n","tournament_data = pd.read_parquet(f'numerai_tournament_data_{current_round}.parquet')\n","print(\"reading validation_data\")\n","validation_data = pd.read_parquet('numerai_validation_data.parquet')\n","print(\"reading example_predictions\")\n","example_preds = pd.read_parquet(f'example_predictions_{current_round}.parquet')\n","print(\"reading example_validaton_predictions\")\n","validation_example_preds = pd.read_parquet('example_validation_predictions.parquet')\n","# set the example predictions\n","validation_data[EXAMPLE_PREDS_COL] = validation_example_preds[\"prediction\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e_vhvYyDE3jA"},"source":["\n","# check for nans and fill nans\n","print(\"checking for nans in the tournament data\")\n","if tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum().sum():\n","    cols_w_nan = tournament_data.loc[tournament_data[\"data_type\"] == \"live\", feature_cols].isna().sum()\n","    total_rows = tournament_data[tournament_data[\"data_type\"] == \"live\"]\n","    print(f\"Number of nans per column this week: {cols_w_nan[cols_w_nan > 0]}\")\n","    print(f\"out of {total_rows} total rows\")\n","    print(f\"filling nans with 0.5\")\n","    tournament_data.loc[:, feature_cols].fillna(0.5, inplace=True)\n","else:\n","    print(\"No nans in the features this week!\")\n","\n","\n","pred_cols = set()\n","ensemble_cols = set()\n","for target in targets:\n","    gc.collect()\n","    model_name = f\"model_{target}\"\n","    print(f\"loading {model_name}\")\n","    model = load_model(model_name)\n","    if not model:\n","        raise ValueError(f\"{model_name} is not trained yet!\")\n","\n","    model_expected_features = model.booster_.feature_name()\n","    if set(model_expected_features) != set(feature_cols):\n","        print(f\"New features are available! Might want to retrain model {model_name}.\")\n","    print(f\"predicting tournament and validation for {model_name}\")\n","    validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(validation_data.loc[:, model_expected_features])\n","    tournament_data.loc[:, f\"preds_{model_name}\"] = model.predict(tournament_data.loc[:, model_expected_features])\n","\n","    # do different neutralizations\n","    # neutralize our predictions to the riskiest features only\n","    print(\"neutralizing to riskiest_50 for validation and tournament\")\n","    validation_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=validation_data,\n","                                                                            columns=[f\"preds_{model_name}\"],\n","                                                                            neutralizers=riskiest_features,\n","                                                                            proportion=1.0,\n","                                                                            normalize=True,\n","                                                                            era_col=ERA_COL)[f\"preds_{model_name}\"]\n","    tournament_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(df=tournament_data,\n","                                                                            columns=[f\"preds_{model_name}\"],\n","                                                                            neutralizers=riskiest_features,\n","                                                                            proportion=1.0,\n","                                                                            normalize=True,\n","                                                                            era_col=ERA_COL)[f\"preds_{model_name}\"]\n","\n","    pred_cols.add(f\"preds_{model_name}\")\n","    pred_cols.add(f\"preds_{model_name}_neutral_riskiest_50\")\n","\n","# make ensembles for val and tournament\n","print('creating ensembles for tournament and validation')\n","validation_data[\"ensemble_neutral_riskiest_50\"] = sum(\n","    [validation_data[pred_col] for pred_col in pred_cols if pred_col.endswith(\"neutral_riskiest_50\")]).rank(\n","    pct=True)\n","tournament_data[\"ensemble_neutral_riskiest_50\"] = sum(\n","    [tournament_data[pred_col] for pred_col in pred_cols if pred_col.endswith(\"neutral_riskiest_50\")]).rank(\n","    pct=True)\n","ensemble_cols.add(\"ensemble_neutral_riskiest_50\")\n","\n","validation_data[\"ensemble_not_neutral\"] = sum(\n","    [validation_data[pred_col] for pred_col in pred_cols if \"neutral\" not in pred_col]).rank(pct=True)\n","tournament_data[\"ensemble_not_neutral\"] = sum(\n","    [tournament_data[pred_col] for pred_col in pred_cols if \"neutral\" not in pred_col]).rank(pct=True)\n","ensemble_cols.add(\"ensemble_not_neutral\")\n","\n","validation_data[\"ensemble_all\"] = sum([validation_data[pred_col] for pred_col in pred_cols]).rank(pct=True)\n","tournament_data[\"ensemble_all\"] = sum([tournament_data[pred_col] for pred_col in pred_cols]).rank(pct=True)\n","\n","ensemble_cols.add(\"ensemble_neutral_riskiest_50\")\n","ensemble_cols.add(\"ensemble_not_neutral\")\n","ensemble_cols.add(\"ensemble_all\")\n","\n","gc.collect()\n","print(\"getting final validation stats\")\n","# get our final validation stats for our chosen model\n","validation_stats = validation_metrics(validation_data, [best_pred_col], example_col=EXAMPLE_PREDS_COL,\n","                                      fast_mode=False)\n","print(validation_stats.to_markdown())\n","\n","# rename best model to prediction and rank from 0 to 1 to meet diagnostic/submission file requirements\n","validation_data[\"prediction\"] = validation_data[best_pred_col].rank(pct=True)\n","tournament_data[\"prediction\"] = tournament_data[best_pred_col].rank(pct=True)\n","validation_data[\"prediction\"].to_csv(f\"prediction_files/validation_predictions_{current_round}.csv\", index=True)\n","tournament_data[\"prediction\"].to_csv(f\"prediction_files/tournament_predictions_{current_round}.csv\", index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZ9r8B1ME3kv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CyhliUDeE3md"},"source":[""],"execution_count":null,"outputs":[]}]}