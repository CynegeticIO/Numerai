{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"XX_XX_The_Model_9_FisaGol.ipynb","provenance":[{"file_id":"1qZ9MXaezKO4_uLd8Clq4HSkd9y2-DVvU","timestamp":1608315596921},{"file_id":"1k0Iv2X-_z35Pbf1tIBCUTovWRO2O1aRJ","timestamp":1607364747831},{"file_id":"1dWjWFBakLacyJ1_C8l6ZituGItKWezJE","timestamp":1607177919869},{"file_id":"https://github.com/parmarsuraj99/numerai-guides/blob/master/better_evaluation/Numerai_evaluate_better.ipynb","timestamp":1606918739349}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5e1b2279843244b08695e8ea889dda19":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_831163c2f2fb446cb6323161951cf020","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b3f37e1f21f84cb5a2a90cc6d0a0af2a","IPY_MODEL_9a76a404e98a4283a1919778e29d0aac"]}},"831163c2f2fb446cb6323161951cf020":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b3f37e1f21f84cb5a2a90cc6d0a0af2a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3c5b4c39a9e2439ca4c2c3aa1beeec1b","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":335,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":335,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6551a66e319f422a97e40397c8cefbf9"}},"9a76a404e98a4283a1919778e29d0aac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0c15ddc8b1dc47f482606d2347108bdb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 335/335 [47:07&lt;00:00,  8.44s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_45ffc73f2b0a4ec1ac9b2e85760f5877"}},"3c5b4c39a9e2439ca4c2c3aa1beeec1b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6551a66e319f422a97e40397c8cefbf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0c15ddc8b1dc47f482606d2347108bdb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"45ffc73f2b0a4ec1ac9b2e85760f5877":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"GjYh7u4jsWul"},"source":["Created by Joan-Marc Fisa\n","\n","- Numerai: [FisaGol](https://numer.ai/fisagol)\n","\n","- Twitter: [@fisagol](https://twitter.com/fisagol)\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DiVYHQNwKsPc","executionInfo":{"status":"ok","timestamp":1621151045835,"user_tz":-120,"elapsed":21560,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"cacf588f-1177-406c-bb7d-5568660f8978"},"source":["from google.colab import drive\n","drive.mount('drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dukzbOx5YPL2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618682821785,"user_tz":-120,"elapsed":9279,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"4e820174-f0f6-447a-ec1d-9d2469fdc4a7"},"source":["!pip install tensorflow\n","!pip install numerapi\n","!pip install vecstack;"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting numerapi\n","  Downloading https://files.pythonhosted.org/packages/fd/96/ebdbaff5a2fef49b212e4f40634166f59e45462a768c0136d148f00255c5/numerapi-2.4.5-py3-none-any.whl\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from numerapi) (2018.9)\n","Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.7/dist-packages (from numerapi) (4.41.1)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from numerapi) (7.1.2)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from numerapi) (2.23.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->numerapi) (1.15.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->numerapi) (1.24.3)\n","Installing collected packages: numerapi\n","Successfully installed numerapi-2.4.5\n","Collecting vecstack\n","  Downloading https://files.pythonhosted.org/packages/d0/a1/b9a1e9e9e5a12078da1ab9788c7885e4c745358f7e57d5f94d9db6a4e898/vecstack-0.4.0.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from vecstack) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from vecstack) (1.4.1)\n","Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from vecstack) (0.22.2.post1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->vecstack) (1.0.1)\n","Building wheels for collected packages: vecstack\n","  Building wheel for vecstack (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for vecstack: filename=vecstack-0.4.0-cp37-none-any.whl size=19877 sha256=2eaf9bbcbe2a05958ddfb460c66497c3e9d2101186914ecab3c5a49f6f0e54d5\n","  Stored in directory: /root/.cache/pip/wheels/5f/bb/4e/f6488433d53bc0684673d6845e5bf11a25240577c8151c140e\n","Successfully built vecstack\n","Installing collected packages: vecstack\n","Successfully installed vecstack-0.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ohVnfTReDQZH"},"source":["import tensorflow as tf\n","if tf.test.gpu_device_name():\n","    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n","else:\n","    print(\"Please install GPU version of TF\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d_TwWD7YXpjL","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1618682827656,"user_tz":-120,"elapsed":2817,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"2b3c9f06-e3a3-4b06-e901-8de6d7bc53cd"},"source":["import os\n","import pathlib\n","import numpy as np\n","import pandas as pd\n","import scipy.stats\n","import tensorflow as tf\n","import joblib\n","from tqdm.notebook import tqdm\n","\n","'''\n","#################### MDO ###########################\n","\n","import torch\n","from torch.nn import Linear\n","from torch.nn import Sequential\n","from torch.functional import F\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n#################### MDO ###########################\\n\\nimport torch\\nfrom torch.nn import Linear\\nfrom torch.nn import Sequential\\nfrom torch.functional import F\\n'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"si8i1PZL7pkN"},"source":["NUMERAI_S3_BUCKET_URL = \"https://numerai-public-datasets.s3-us-west-2.amazonaws.com\"\n","\n","EXAMPLE_PREDS_URL = NUMERAI_S3_BUCKET_URL + \"/latest_numerai_example_predictions_data.csv.xz\"\n","TOURNAMENT_DATA_URL = NUMERAI_S3_BUCKET_URL + \"/latest_numerai_tournament_data.csv.xz\"\n","TEST_DATA = NUMERAI_S3_BUCKET_URL + \"/latest_numerai_max_test_era_data.csv.xz\"\n","\n","###IMPORTANT! DELETE THE FILE BELOW IF YOU CHANGE MODELS! OTHERWISE, RENAME THE FILE FOR YOUR VARIOUS MODELS###\n","LM_CACHE_FILE = pathlib.Path(\"neutralization.cache.joblib\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZJEeGsiZ7q5m","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1618682842897,"user_tz":-120,"elapsed":884,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"1f0d10df-9baf-4d87-cf67-9ac1e9761a48"},"source":["\n","@tf.function(experimental_relax_shapes=True, experimental_compile=True)\n","def exposures(x, y):\n","    x = x - tf.math.reduce_mean(x, axis=0)\n","    x = x / tf.norm(x, axis=0)\n","    y = y - tf.math.reduce_mean(y, axis=0)\n","    y = y / tf.norm(y, axis=0)\n","    return tf.matmul(x, y, transpose_a=True)\n","\n","#################### MDO ###########################\n","'''\n","def exposures(x, y):\n","    x = x - x.mean(dim=0)\n","    x = x / x.norm(dim=0)\n","    y = y - y.mean(dim=0)\n","    y = y / y.norm(dim=0)\n","    return torch.matmul(x.T, y)\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\ndef exposures(x, y):\\n    x = x - x.mean(dim=0)\\n    x = x / x.norm(dim=0)\\n    y = y - y.mean(dim=0)\\n    y = y / y.norm(dim=0)\\n    return torch.matmul(x.T, y)\\n'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"000ByXRS7sI9"},"source":["@tf.function(experimental_relax_shapes=True)\n","def train_loop_body(model, feats, pred, target_exps):\n","    with tf.GradientTape() as tape:\n","        exps = exposures(feats, pred[:, None] - model(feats, training=True))\n","        loss = tf.reduce_sum(tf.nn.relu(tf.nn.relu(exps) - tf.nn.relu(target_exps)) +\n","                             tf.nn.relu(tf.nn.relu(-exps) - tf.nn.relu(-target_exps)))\n","    return loss, tape.gradient(loss, model.trainable_variables)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xr5gfXGQ7tjD"},"source":["def train_loop(model, optimizer, feats, pred, target_exps, era):\n","    for i in range(1000000):\n","        loss, grads = train_loop_body(model, feats, pred, target_exps)\n","        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","        if loss < 1e-7:\n","            break\n","        if i % 10000 == 0:\n","            tqdm.write(f'era: {era[3:]} loss: {loss:0.7f}', end='\\r')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lq6iUj3m7uZB","colab":{"base_uri":"https://localhost:8080/","height":107},"executionInfo":{"status":"ok","timestamp":1618682854585,"user_tz":-120,"elapsed":720,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"6cc72278-14e3-432b-eeec-b835abc1519b"},"source":["def reduce_exposure(prediction, features, max_exp, era, weights=None):\n","    model = tf.keras.models.Sequential([\n","        tf.keras.layers.Input(310),\n","        tf.keras.experimental.LinearModel(use_bias=False),\n","    ])\n","    feats = tf.convert_to_tensor(features - 0.5, dtype=tf.float32)\n","    pred = tf.convert_to_tensor(prediction, dtype=tf.float32)\n","    if weights is None:\n","        optimizer = tf.keras.optimizers.Adamax()\n","        start_exp = exposures(feats, pred[:, None])\n","        target_exps = tf.clip_by_value(start_exp, -max_exp, max_exp)\n","        train_loop(model, optimizer, feats, pred, target_exps, era)\n","    else:\n","        model.set_weights(weights)\n","    return pred[:,None] - model(feats), model.get_weights()\n","\n","\n","###################   MDO    ########################################\n","\n","'''\n","def reduce_exposure(prediction, features, max_exp):\n","    # linear model of features that will be used to partially neutralize predictions\n","    lin = Linear(features.shape[1],  1, bias=False)\n","    lin.weight.data.fill_(0.)\n","    model = Sequential(lin)\n","    optimizer = torch.optim.Adamax(model.parameters(), lr=1e-4)\n","    feats = torch.tensor(np.float32(features)-.5)\n","    pred = torch.tensor(np.float32(prediction))\n","    start_exp = exposures(feats, pred[:,None])\n","    # set target exposure for each feature to be <= current exposure\n","    # if current exposure is less than max_exp, or <= max_exp if  \n","    # current exposure is > max_exp\n","    targ_exp = torch.clamp(start_exp, -max_exp, max_exp)\n","\n","    for i in range(100000):\n","        optimizer.zero_grad()\n","        # calculate feature exposures of current linear neutralization\n","        exps = exposures(feats, pred[:,None]-model(feats))\n","        # loss is positive when any exposures exceed their target\n","        loss = (F.relu(F.relu(exps)-F.relu(targ_exp)) + F.relu(F.relu(-exps)-F.relu(-targ_exp))).sum()\n","        print(f'       loss: {loss:0.7f}', end='\\r')\n","        if loss < 1e-7:\n","            neutralizer = [p.detach().numpy() for p in model.parameters()]\n","            neutralized_pred = pred[:,None]-model(feats)\n","            break\n","        loss.backward()\n","        optimizer.step()\n","    return neutralized_pred, neutralizer\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\ndef reduce_exposure(prediction, features, max_exp):\\n    # linear model of features that will be used to partially neutralize predictions\\n    lin = Linear(features.shape[1],  1, bias=False)\\n    lin.weight.data.fill_(0.)\\n    model = Sequential(lin)\\n    optimizer = torch.optim.Adamax(model.parameters(), lr=1e-4)\\n    feats = torch.tensor(np.float32(features)-.5)\\n    pred = torch.tensor(np.float32(prediction))\\n    start_exp = exposures(feats, pred[:,None])\\n    # set target exposure for each feature to be <= current exposure\\n    # if current exposure is less than max_exp, or <= max_exp if  \\n    # current exposure is > max_exp\\n    targ_exp = torch.clamp(start_exp, -max_exp, max_exp)\\n\\n    for i in range(100000):\\n        optimizer.zero_grad()\\n        # calculate feature exposures of current linear neutralization\\n        exps = exposures(feats, pred[:,None]-model(feats))\\n        # loss is positive when any exposures exceed their target\\n        loss = (F.relu(F.relu(exps)-F.relu(targ_exp)) + F.relu(F.relu(-exps)-F.relu(-targ_exp))).sum()\\n        print(f'       loss: {loss:0.7f}', end='\\r')\\n        if loss < 1e-7:\\n            neutralizer = [p.detach().numpy() for p in model.parameters()]\\n            neutralized_pred = pred[:,None]-model(feats)\\n            break\\n        loss.backward()\\n        optimizer.step()\\n    return neutralized_pred, neutralizer\\n\""]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"yBiyO9uN7vUy","colab":{"base_uri":"https://localhost:8080/","height":107},"executionInfo":{"status":"ok","timestamp":1618682863671,"user_tz":-120,"elapsed":719,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"3d59f2b9-0f7a-4768-e94d-ecd320ed03ba"},"source":["def reduce_all_exposures(df, column=[\"prediction\"], neutralizers=None,\n","                                     normalize=True,\n","                                     gaussianize=True,\n","                                     era_col=\"era\",\n","                                     max_exp=0.1): ###<-----SELECT YOUR MAXIMUM FEATURE EXPOSURE HERE###\n","    if neutralizers is None:\n","        neutralizers = [x for x in df.columns if x.startswith(\"feature\")]\n","    neutralized = []\n","    if LM_CACHE_FILE.is_file():\n","        cache = joblib.load(LM_CACHE_FILE)\n","        # Remove weights for eraX if we'd accidentally saved it in the past.\n","        cache.pop(\"eraX\", None)\n","    else:\n","        cache = {}\n","    for era in tqdm(df[era_col].unique()):\n","        tqdm.write(era, end='\\r')\n","        df_era = df[df[era_col] == era]\n","        scores = df_era[column].values\n","        exposure_values = df_era[neutralizers].values\n","\n","        if normalize:\n","            scores2 = []\n","            for x in scores.T:\n","                x = (scipy.stats.rankdata(x, method='ordinal') - .5) / len(x)\n","                if gaussianize:\n","                    x = scipy.stats.norm.ppf(x)\n","                scores2.append(x)\n","            scores = np.array(scores2)[0]\n","\n","        scores, weights = reduce_exposure(scores, exposure_values,\n","                                          max_exp, era, cache.get(era))\n","        if era not in cache and era != \"eraX\":\n","            cache[era] = weights\n","            joblib.dump(cache, LM_CACHE_FILE)\n","        scores /= tf.math.reduce_std(scores)\n","        scores -= tf.reduce_min(scores)\n","        scores /= tf.reduce_max(scores)\n","        neutralized.append(scores.numpy())\n","\n","    predictions = pd.DataFrame(np.concatenate(neutralized),\n","                               columns=column, index=df.index)\n","    return predictions\n","\n","\n","############################   MDO   ######################################\n","'''\n","def reduce_all_exposures(df, column, neutralizers=[],\n","                                     normalize=True,\n","                                     gaussianize=True,\n","                                     era_col=\"era\",\n","                                     max_exp=0.1):\n","    unique_eras = df[era_col].unique()\n","    computed = []\n","    for u in unique_eras:\n","        print(u, '\\r')\n","        df_era = df[df[era_col] == u]\n","        scores = df_era[column].values\n","        exposure_values = df_era[neutralizers].values\n","        \n","        if normalize:\n","            scores2 = []\n","            for x in scores.T:\n","                x = (scipy.stats.rankdata(x, method='ordinal') - .5) / len(x)\n","                if gaussianize:\n","                    x = scipy.stats.norm.ppf(x)\n","                scores2.append(x)\n","            scores = np.array(scores2)[0]\n","\n","        scores, neut = reduce_exposure(scores, exposure_values, max_exp)\n","\n","        scores /= scores.std()\n","\n","        computed.append(scores.detach().numpy())\n","\n","    return pd.DataFrame(np.concatenate(computed), columns=column, index=df.index)\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\ndef reduce_all_exposures(df, column, neutralizers=[],\\n                                     normalize=True,\\n                                     gaussianize=True,\\n                                     era_col=\"era\",\\n                                     max_exp=0.1):\\n    unique_eras = df[era_col].unique()\\n    computed = []\\n    for u in unique_eras:\\n        print(u, \\'\\r\\')\\n        df_era = df[df[era_col] == u]\\n        scores = df_era[column].values\\n        exposure_values = df_era[neutralizers].values\\n        \\n        if normalize:\\n            scores2 = []\\n            for x in scores.T:\\n                x = (scipy.stats.rankdata(x, method=\\'ordinal\\') - .5) / len(x)\\n                if gaussianize:\\n                    x = scipy.stats.norm.ppf(x)\\n                scores2.append(x)\\n            scores = np.array(scores2)[0]\\n\\n        scores, neut = reduce_exposure(scores, exposure_values, max_exp)\\n\\n        scores /= scores.std()\\n\\n        computed.append(scores.detach().numpy())\\n\\n    return pd.DataFrame(np.concatenate(computed), columns=column, index=df.index)\\n'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"g17sABozXppm"},"source":["#If CUDA isn't set up properly for Tensorflow, then at least maximize the number of threads available for CPU\n","if not tf.config.list_physical_devices('GPU'):  # No GPU(s) found\n","    tf.config.threading.set_inter_op_parallelism_threads(2)\n","    tf.config.threading.set_intra_op_parallelism_threads(os.cpu_count() // 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QE-xxAWwXptV","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"ok","timestamp":1618682880771,"user_tz":-120,"elapsed":5902,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"0fe1a579-d545-4979-fcc2-89bd237021b1"},"source":["#read-in or download the example predictions\n","exp_df = pd.read_csv(EXAMPLE_PREDS_URL, index_col=0)\n","\n","exp_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prediction</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>n0003aa52cab36c2</th>\n","      <td>0.49</td>\n","    </tr>\n","    <tr>\n","      <th>n000920ed083903f</th>\n","      <td>0.49</td>\n","    </tr>\n","    <tr>\n","      <th>n0038e640522c4a6</th>\n","      <td>0.53</td>\n","    </tr>\n","    <tr>\n","      <th>n004ac94a87dc54b</th>\n","      <td>0.51</td>\n","    </tr>\n","    <tr>\n","      <th>n0052fe97ea0c05f</th>\n","      <td>0.50</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  prediction\n","id                          \n","n0003aa52cab36c2        0.49\n","n000920ed083903f        0.49\n","n0038e640522c4a6        0.53\n","n004ac94a87dc54b        0.51\n","n0052fe97ea0c05f        0.50"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"E3BUnkOUX7qk","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"ok","timestamp":1618682880774,"user_tz":-120,"elapsed":5900,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"9a1c75ac-c2d4-4af9-d5d5-b7504096a95b"},"source":["exp_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prediction</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>n0003aa52cab36c2</th>\n","      <td>0.49</td>\n","    </tr>\n","    <tr>\n","      <th>n000920ed083903f</th>\n","      <td>0.49</td>\n","    </tr>\n","    <tr>\n","      <th>n0038e640522c4a6</th>\n","      <td>0.53</td>\n","    </tr>\n","    <tr>\n","      <th>n004ac94a87dc54b</th>\n","      <td>0.51</td>\n","    </tr>\n","    <tr>\n","      <th>n0052fe97ea0c05f</th>\n","      <td>0.50</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  prediction\n","id                          \n","n0003aa52cab36c2        0.49\n","n000920ed083903f        0.49\n","n0038e640522c4a6        0.53\n","n004ac94a87dc54b        0.51\n","n0052fe97ea0c05f        0.50"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"i0Gs9bJjXv-7","colab":{"base_uri":"https://localhost:8080/","height":284},"executionInfo":{"status":"ok","timestamp":1618682974188,"user_tz":-120,"elapsed":99310,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"c8a06b41-605a-4532-d487-465e16652ab2"},"source":["#download the tournament data\n","tournament_df = pd.read_csv(TOURNAMENT_DATA_URL, index_col=0)\n","tournament_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>era</th>\n","      <th>data_type</th>\n","      <th>feature_intelligence1</th>\n","      <th>feature_intelligence2</th>\n","      <th>feature_intelligence3</th>\n","      <th>feature_intelligence4</th>\n","      <th>feature_intelligence5</th>\n","      <th>feature_intelligence6</th>\n","      <th>feature_intelligence7</th>\n","      <th>feature_intelligence8</th>\n","      <th>feature_intelligence9</th>\n","      <th>feature_intelligence10</th>\n","      <th>feature_intelligence11</th>\n","      <th>feature_intelligence12</th>\n","      <th>feature_charisma1</th>\n","      <th>feature_charisma2</th>\n","      <th>feature_charisma3</th>\n","      <th>feature_charisma4</th>\n","      <th>feature_charisma5</th>\n","      <th>feature_charisma6</th>\n","      <th>feature_charisma7</th>\n","      <th>feature_charisma8</th>\n","      <th>feature_charisma9</th>\n","      <th>feature_charisma10</th>\n","      <th>feature_charisma11</th>\n","      <th>feature_charisma12</th>\n","      <th>feature_charisma13</th>\n","      <th>feature_charisma14</th>\n","      <th>feature_charisma15</th>\n","      <th>feature_charisma16</th>\n","      <th>feature_charisma17</th>\n","      <th>feature_charisma18</th>\n","      <th>feature_charisma19</th>\n","      <th>feature_charisma20</th>\n","      <th>feature_charisma21</th>\n","      <th>feature_charisma22</th>\n","      <th>feature_charisma23</th>\n","      <th>feature_charisma24</th>\n","      <th>feature_charisma25</th>\n","      <th>feature_charisma26</th>\n","      <th>...</th>\n","      <th>feature_wisdom8</th>\n","      <th>feature_wisdom9</th>\n","      <th>feature_wisdom10</th>\n","      <th>feature_wisdom11</th>\n","      <th>feature_wisdom12</th>\n","      <th>feature_wisdom13</th>\n","      <th>feature_wisdom14</th>\n","      <th>feature_wisdom15</th>\n","      <th>feature_wisdom16</th>\n","      <th>feature_wisdom17</th>\n","      <th>feature_wisdom18</th>\n","      <th>feature_wisdom19</th>\n","      <th>feature_wisdom20</th>\n","      <th>feature_wisdom21</th>\n","      <th>feature_wisdom22</th>\n","      <th>feature_wisdom23</th>\n","      <th>feature_wisdom24</th>\n","      <th>feature_wisdom25</th>\n","      <th>feature_wisdom26</th>\n","      <th>feature_wisdom27</th>\n","      <th>feature_wisdom28</th>\n","      <th>feature_wisdom29</th>\n","      <th>feature_wisdom30</th>\n","      <th>feature_wisdom31</th>\n","      <th>feature_wisdom32</th>\n","      <th>feature_wisdom33</th>\n","      <th>feature_wisdom34</th>\n","      <th>feature_wisdom35</th>\n","      <th>feature_wisdom36</th>\n","      <th>feature_wisdom37</th>\n","      <th>feature_wisdom38</th>\n","      <th>feature_wisdom39</th>\n","      <th>feature_wisdom40</th>\n","      <th>feature_wisdom41</th>\n","      <th>feature_wisdom42</th>\n","      <th>feature_wisdom43</th>\n","      <th>feature_wisdom44</th>\n","      <th>feature_wisdom45</th>\n","      <th>feature_wisdom46</th>\n","      <th>target</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>n0003aa52cab36c2</th>\n","      <td>era121</td>\n","      <td>validation</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.0</td>\n","      <td>0.75</td>\n","      <td>0.5</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.5</td>\n","      <td>0.25</td>\n","      <td>0.0</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.0</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.25</td>\n","      <td>0.5</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.5</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.5</td>\n","      <td>1.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","    </tr>\n","    <tr>\n","      <th>n000920ed083903f</th>\n","      <td>era121</td>\n","      <td>validation</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>0.5</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.0</td>\n","      <td>0.75</td>\n","      <td>0.5</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>...</td>\n","      <td>0.50</td>\n","      <td>0.5</td>\n","      <td>0.25</td>\n","      <td>1.0</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.5</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.5</td>\n","      <td>0.5</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>n0038e640522c4a6</th>\n","      <td>era121</td>\n","      <td>validation</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>1.0</td>\n","      <td>1.00</td>\n","      <td>1.0</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>0.5</td>\n","      <td>1.00</td>\n","      <td>1.0</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>0.5</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>...</td>\n","      <td>0.25</td>\n","      <td>0.5</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.5</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>n004ac94a87dc54b</th>\n","      <td>era121</td>\n","      <td>validation</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>1.0</td>\n","      <td>0.75</td>\n","      <td>0.0</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>1.0</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>n0052fe97ea0c05f</th>\n","      <td>era121</td>\n","      <td>validation</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>1.0</td>\n","      <td>0.50</td>\n","      <td>0.5</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.5</td>\n","      <td>0.50</td>\n","      <td>1.0</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.5</td>\n","      <td>0.50</td>\n","      <td>0.0</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>1.0</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 313 columns</p>\n","</div>"],"text/plain":["                     era   data_type  ...  feature_wisdom46  target\n","id                                    ...                          \n","n0003aa52cab36c2  era121  validation  ...              0.00    0.25\n","n000920ed083903f  era121  validation  ...              0.50    0.50\n","n0038e640522c4a6  era121  validation  ...              0.00    1.00\n","n004ac94a87dc54b  era121  validation  ...              0.25    0.50\n","n0052fe97ea0c05f  era121  validation  ...              1.00    0.75\n","\n","[5 rows x 313 columns]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":470},"id":"w4Q7l5HxX691","executionInfo":{"status":"ok","timestamp":1618682975660,"user_tz":-120,"elapsed":100779,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"a166f6d4-5888-4102-cda1-29eb480469ce"},"source":["\n","tournament_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>era</th>\n","      <th>data_type</th>\n","      <th>feature_intelligence1</th>\n","      <th>feature_intelligence2</th>\n","      <th>feature_intelligence3</th>\n","      <th>feature_intelligence4</th>\n","      <th>feature_intelligence5</th>\n","      <th>feature_intelligence6</th>\n","      <th>feature_intelligence7</th>\n","      <th>feature_intelligence8</th>\n","      <th>feature_intelligence9</th>\n","      <th>feature_intelligence10</th>\n","      <th>feature_intelligence11</th>\n","      <th>feature_intelligence12</th>\n","      <th>feature_charisma1</th>\n","      <th>feature_charisma2</th>\n","      <th>feature_charisma3</th>\n","      <th>feature_charisma4</th>\n","      <th>feature_charisma5</th>\n","      <th>feature_charisma6</th>\n","      <th>feature_charisma7</th>\n","      <th>feature_charisma8</th>\n","      <th>feature_charisma9</th>\n","      <th>feature_charisma10</th>\n","      <th>feature_charisma11</th>\n","      <th>feature_charisma12</th>\n","      <th>feature_charisma13</th>\n","      <th>feature_charisma14</th>\n","      <th>feature_charisma15</th>\n","      <th>feature_charisma16</th>\n","      <th>feature_charisma17</th>\n","      <th>feature_charisma18</th>\n","      <th>feature_charisma19</th>\n","      <th>feature_charisma20</th>\n","      <th>feature_charisma21</th>\n","      <th>feature_charisma22</th>\n","      <th>feature_charisma23</th>\n","      <th>feature_charisma24</th>\n","      <th>feature_charisma25</th>\n","      <th>feature_charisma26</th>\n","      <th>...</th>\n","      <th>feature_wisdom8</th>\n","      <th>feature_wisdom9</th>\n","      <th>feature_wisdom10</th>\n","      <th>feature_wisdom11</th>\n","      <th>feature_wisdom12</th>\n","      <th>feature_wisdom13</th>\n","      <th>feature_wisdom14</th>\n","      <th>feature_wisdom15</th>\n","      <th>feature_wisdom16</th>\n","      <th>feature_wisdom17</th>\n","      <th>feature_wisdom18</th>\n","      <th>feature_wisdom19</th>\n","      <th>feature_wisdom20</th>\n","      <th>feature_wisdom21</th>\n","      <th>feature_wisdom22</th>\n","      <th>feature_wisdom23</th>\n","      <th>feature_wisdom24</th>\n","      <th>feature_wisdom25</th>\n","      <th>feature_wisdom26</th>\n","      <th>feature_wisdom27</th>\n","      <th>feature_wisdom28</th>\n","      <th>feature_wisdom29</th>\n","      <th>feature_wisdom30</th>\n","      <th>feature_wisdom31</th>\n","      <th>feature_wisdom32</th>\n","      <th>feature_wisdom33</th>\n","      <th>feature_wisdom34</th>\n","      <th>feature_wisdom35</th>\n","      <th>feature_wisdom36</th>\n","      <th>feature_wisdom37</th>\n","      <th>feature_wisdom38</th>\n","      <th>feature_wisdom39</th>\n","      <th>feature_wisdom40</th>\n","      <th>feature_wisdom41</th>\n","      <th>feature_wisdom42</th>\n","      <th>feature_wisdom43</th>\n","      <th>feature_wisdom44</th>\n","      <th>feature_wisdom45</th>\n","      <th>feature_wisdom46</th>\n","      <th>target</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>n0003aa52cab36c2</th>\n","      <td>era121</td>\n","      <td>validation</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.0</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","    </tr>\n","    <tr>\n","      <th>n000920ed083903f</th>\n","      <td>era121</td>\n","      <td>validation</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>0.5</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>...</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>n0038e640522c4a6</th>\n","      <td>era121</td>\n","      <td>validation</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>1.0</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>...</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","    </tr>\n","    <tr>\n","      <th>n004ac94a87dc54b</th>\n","      <td>era121</td>\n","      <td>validation</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>0.0</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","    </tr>\n","    <tr>\n","      <th>n0052fe97ea0c05f</th>\n","      <td>era121</td>\n","      <td>validation</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>1.0</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>nffb0e4e60b166ea</th>\n","      <td>eraX</td>\n","      <td>live</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.5</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>...</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>nffc2bf82af58f1c</th>\n","      <td>eraX</td>\n","      <td>live</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.0</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>nffea596c525e547</th>\n","      <td>eraX</td>\n","      <td>live</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.5</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>...</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>nfff3afb5a848263</th>\n","      <td>eraX</td>\n","      <td>live</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.0</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>nfff5bbb1bb13719</th>\n","      <td>eraX</td>\n","      <td>live</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.00</td>\n","      <td>1.0</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>...</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.00</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.25</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.00</td>\n","      <td>0.50</td>\n","      <td>0.25</td>\n","      <td>0.25</td>\n","      <td>0.75</td>\n","      <td>0.75</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.75</td>\n","      <td>0.50</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1698481 rows × 313 columns</p>\n","</div>"],"text/plain":["                     era   data_type  ...  feature_wisdom46  target\n","id                                    ...                          \n","n0003aa52cab36c2  era121  validation  ...              0.00    0.25\n","n000920ed083903f  era121  validation  ...              0.50    0.50\n","n0038e640522c4a6  era121  validation  ...              0.00    1.00\n","n004ac94a87dc54b  era121  validation  ...              0.25    0.50\n","n0052fe97ea0c05f  era121  validation  ...              1.00    0.75\n","...                  ...         ...  ...               ...     ...\n","nffb0e4e60b166ea    eraX        live  ...              0.50     NaN\n","nffc2bf82af58f1c    eraX        live  ...              1.00     NaN\n","nffea596c525e547    eraX        live  ...              0.75     NaN\n","nfff3afb5a848263    eraX        live  ...              0.50     NaN\n","nfff5bbb1bb13719    eraX        live  ...              1.00     NaN\n","\n","[1698481 rows x 313 columns]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"t9hY7IcEXwzd"},"source":["#merge them together\n","full_df = pd.merge(tournament_df, exp_df, left_index=True, right_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["5e1b2279843244b08695e8ea889dda19","831163c2f2fb446cb6323161951cf020","b3f37e1f21f84cb5a2a90cc6d0a0af2a","9a76a404e98a4283a1919778e29d0aac","3c5b4c39a9e2439ca4c2c3aa1beeec1b","6551a66e319f422a97e40397c8cefbf9","0c15ddc8b1dc47f482606d2347108bdb","45ffc73f2b0a4ec1ac9b2e85760f5877"]},"id":"Pie7We0wYFFN","executionInfo":{"status":"ok","timestamp":1618685808772,"user_tz":-120,"elapsed":2933886,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"c01cfb50-42e4-47c4-a252-e7f9a38ecbdc"},"source":["#this cell executes the full script above and neutralizes the predictions to achieve a maximum 0.1 Feature Exposure\n","neutralized_df = reduce_all_exposures(full_df)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e1b2279843244b08695e8ea889dda19","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=335.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mckvB-50k9aX","colab":{"base_uri":"https://localhost:8080/","height":450},"executionInfo":{"status":"ok","timestamp":1618685808773,"user_tz":-120,"elapsed":2933883,"user":{"displayName":"Joan-Marc Fisa Gol","photoUrl":"","userId":"00374894131735723747"}},"outputId":"d02d423b-ae22-40a7-b06a-80d967acaba7"},"source":["neutralized_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>prediction</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>n0003aa52cab36c2</th>\n","      <td>0.320204</td>\n","    </tr>\n","    <tr>\n","      <th>n000920ed083903f</th>\n","      <td>0.308392</td>\n","    </tr>\n","    <tr>\n","      <th>n0038e640522c4a6</th>\n","      <td>0.575278</td>\n","    </tr>\n","    <tr>\n","      <th>n004ac94a87dc54b</th>\n","      <td>0.556190</td>\n","    </tr>\n","    <tr>\n","      <th>n0052fe97ea0c05f</th>\n","      <td>0.409397</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>nffb0e4e60b166ea</th>\n","      <td>0.540612</td>\n","    </tr>\n","    <tr>\n","      <th>nffc2bf82af58f1c</th>\n","      <td>0.623061</td>\n","    </tr>\n","    <tr>\n","      <th>nffea596c525e547</th>\n","      <td>0.686991</td>\n","    </tr>\n","    <tr>\n","      <th>nfff3afb5a848263</th>\n","      <td>0.540267</td>\n","    </tr>\n","    <tr>\n","      <th>nfff5bbb1bb13719</th>\n","      <td>0.762006</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1698481 rows × 1 columns</p>\n","</div>"],"text/plain":["                  prediction\n","id                          \n","n0003aa52cab36c2    0.320204\n","n000920ed083903f    0.308392\n","n0038e640522c4a6    0.575278\n","n004ac94a87dc54b    0.556190\n","n0052fe97ea0c05f    0.409397\n","...                      ...\n","nffb0e4e60b166ea    0.540612\n","nffc2bf82af58f1c    0.623061\n","nffea596c525e547    0.686991\n","nfff3afb5a848263    0.540267\n","nfff5bbb1bb13719    0.762006\n","\n","[1698481 rows x 1 columns]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"f3ny5v8lYH5M"},"source":["#save your prediction file locally\n","neutralized_df.to_csv(\"FisaGol_1.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jk0XXLm50QTY"},"source":["!cp FisaGol_1.csv \"drive/My Drive/PHOENIXSIGMA/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPfBZ1sLqDnn"},"source":["########################### MDO CODE ######################"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v1nAsJFeqDvq"},"source":["########################### MDO CODE ######################"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fTZn_TjPqD0F"},"source":["########################### MDO CODE ######################"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NpNvVlAW2hq_"},"source":["########################### MDO CODE ######################"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CgqypvCqjyIz"},"source":["import numpy as np \n","import pandas as pd \n","from xgboost import XGBRegressor \n","import torch\n","from torch.autograd import grad\n","\n","\n","trainval=pd.read_parquet(\"numerai_training_validation_target_nomi.parquet\")\n","train = trainval[trainval.data_type=='train']\n","\n","target = \"target_nomi\" \n","feature_columns = [c for c in trainval if c.startswith(\"feature\")] \n","\n","# fit an initial model\n","model_init = XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=2000, colsample_bytree=0.1, nthread=6)\n","model_init.fit(train[feature_columns], train[target])\n","\n","# get prediction from initial model as starting point to improve upon\n","base_margin = model_init.predict(train[feature_columns])\n","\n","# get indexes for each era\n","era_idx = [np.where(train.era==uera)[0] for uera in train.era.unique()]\n","\n","\n","# define adjusted sharpe in terms of cost adjusted numerai sharpe\n","def numerai_sharpe(x):\n","    return (x.mean() -0.010415154) / x.std()\n","\n","def skew(x):\n","    mx = x.mean()\n","    m2 = ((x-mx)**2).mean()\n","    m3 = ((x-mx)**3).mean()\n","    return m3/(m2**1.5)    \n","\n","def kurtosis(x):\n","    mx = x.mean()\n","    m4 = ((x-mx)**4).mean()\n","    m2 = ((x-mx)**2).mean()\n","    return (m4/(m2**2))-3\n","\n","def adj_sharpe(x):\n","    return numerai_sharpe(x) * (1 + ((skew(x) / 6) * numerai_sharpe(x)) - ((kurtosis(x) / 24) * (numerai_sharpe(x) ** 2)))\n","\n","# use correlation as the measure of fit\n","def corr(pred, target):\n","    pred_n = pred - pred.mean(dim=0)\n","    pred_n = pred_n / pred_n.norm(dim=0)\n","\n","    target_n = target - target.mean(dim=0)\n","    target_n = target_n / target_n.norm(dim=0)\n","    l = torch.matmul(pred_n.T, target_n)\n","    return l\n","\n","# definte a custom objective for XGBoost\n","def adj_sharpe_obj(ytrue, ypred):\n","    # convert to pytorch tensors\n","    ypred_th = torch.tensor(ypred, requires_grad=True)\n","    ytrue_th = torch.tensor(ytrue)\n","    all_corrs = []\n","\n","    # get correlations in each era\n","    for ee in era_idx:\n","        score = corr(ypred_th[ee], ytrue_th[ee])\n","        all_corrs.append(score)\n","\n","    all_corrs = torch.stack(all_corrs)\n","\n","    # calculate adjusted sharpe using correlations\n","    loss = -adj_sharpe(all_corrs)\n","    print(f'Current loss:{loss}')\n","\n","    # calculate gradient and convert to numpy\n","    loss_grads = grad(loss, ypred_th, create_graph=True)[0]\n","    loss_grads = loss_grads.detach().numpy()\n","\n","    # return gradient and ones instead of Hessian diagonal\n","    return loss_grads, np.ones(loss_grads.shape)\n","\n","\n","model_adj_sharpe = XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=200, nthread=6, colsample_bytree=0.1, objective=adj_sharpe_obj)\n","model_adj_sharpe.fit(train[feature_columns], train[target], base_margin=base_margin)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v1OH1E7Rkymp"},"source":["def neutralize(df, target=\"prediction_kazutsugi\", by=None, proportion=1.0):\n","    if by is None:\n","        by = [x for x in df.columns if x.startswith('feature')]\n","\n","    scores = df[target]\n","    exposures = df[by].values\n","\n","    # constant column to make sure the series is completely neutral to exposures\n","    exposures = np.hstack((exposures, np.array([np.mean(scores)] * len(exposures)).reshape(-1, 1)))\n","\n","    scores -= proportion * (exposures @ (np.linalg.pinv(exposures) @ scores.values))\n","    return scores / scores.std()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WtpEWQ4IlBla"},"source":["import torch\n","from torch.nn import Linear\n","from torch.nn import Sequential\n","from torch.functional import F\n","\n","def exposures(x, y):\n","    x = x - x.mean(dim=0)\n","    x = x / x.norm(dim=0)\n","    y = y - y.mean(dim=0)\n","    y = y / y.norm(dim=0)\n","    return torch.matmul(x.T, y)\n","\n","def reduce_exposure(prediction, features, max_exp):\n","    # linear model of features that will be used to partially neutralize predictions\n","    lin = Linear(features.shape[1],  1, bias=False)\n","    lin.weight.data.fill_(0.)\n","    model = Sequential(lin)\n","    optimizer = torch.optim.Adamax(model.parameters(), lr=1e-4)\n","    feats = torch.tensor(np.float32(features)-.5)\n","    pred = torch.tensor(np.float32(prediction))\n","    start_exp = exposures(feats, pred[:,None])\n","    # set target exposure for each feature to be <= current exposure\n","    # if current exposure is less than max_exp, or <= max_exp if  \n","    # current exposure is > max_exp\n","    targ_exp = torch.clamp(start_exp, -max_exp, max_exp)\n","\n","    for i in range(100000):\n","        optimizer.zero_grad()\n","        # calculate feature exposures of current linear neutralization\n","        exps = exposures(feats, pred[:,None]-model(feats))\n","        # loss is positive when any exposures exceed their target\n","        loss = (F.relu(F.relu(exps)-F.relu(targ_exp)) + F.relu(F.relu(-exps)-F.relu(-targ_exp))).sum()\n","        print(f'       loss: {loss:0.7f}', end='\\r')\n","        if loss < 1e-7:\n","            neutralizer = [p.detach().numpy() for p in model.parameters()]\n","            neutralized_pred = pred[:,None]-model(feats)\n","            break\n","        loss.backward()\n","        optimizer.step()\n","    return neutralized_pred, neutralizer\n","\n","def reduce_all_exposures(df, column, neutralizers=[],\n","                                     normalize=True,\n","                                     gaussianize=True,\n","                                     era_col=\"era\",\n","                                     max_exp=0.1):\n","    unique_eras = df[era_col].unique()\n","    computed = []\n","    for u in unique_eras:\n","        print(u, '\\r')\n","        df_era = df[df[era_col] == u]\n","        scores = df_era[column].values\n","        exposure_values = df_era[neutralizers].values\n","        \n","        if normalize:\n","            scores2 = []\n","            for x in scores.T:\n","                x = (scipy.stats.rankdata(x, method='ordinal') - .5) / len(x)\n","                if gaussianize:\n","                    x = scipy.stats.norm.ppf(x)\n","                scores2.append(x)\n","            scores = np.array(scores2)[0]\n","\n","        scores, neut = reduce_exposure(scores, exposure_values, max_exp)\n","\n","        scores /= scores.std()\n","\n","        computed.append(scores.detach().numpy())\n","\n","    return pd.DataFrame(np.concatenate(computed), columns=column, index=df.index)\n","\n","\n","TOURNAMENT_NAME = \"kazutsugi\"\n","PREDICTION_NAME = f\"prediction_{TOURNAMENT_NAME}\"\n","\n","## Get output of your model\n","# data[PREDICTION_NAME] = model.predict(data[feature_names])\n","\n","# reduce feature exposure in each era to max_exp\n","data_rfe_10 = reduce_all_exposures(data,\n","                                   [PREDICTION_NAME],\n","                                   neutralizers=feature_names,\n","                                   era_col=\"era\",\n","                                   max_exp=0.10)\n","\n","# replace prediction with reduced feature exposure prediction and rescale to [0,1]\n","data[PREDICTION_NAME] = data_rfe_10[PREDICTION_NAME]\n","data[PREDICTION_NAME] -= data[PREDICTION_NAME].min()\n","data[PREDICTION_NAME] /= data[PREDICTION_NAME].max()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"23n-2YgJnjDz"},"source":["def pearson_cumsom_loss(y_true, y_pred):\n","    '''\n","    optmize negative pearson coefficient loss\n","    :param y_true:\n","    :param y_pred:\n","    :return:\n","    '''\n","    if isinstance(y_true, pd.Series):\n","        y_true = y_true.values\n","    if isinstance(y_pred, pd.Series):\n","        y_pred = y_pred.values\n","    n = len(y_true)\n","    y_bar = y_true.mean()\n","    yhat_bar = y_pred.mean()\n","    c = 1 / ((y_true - y_bar) ** 2).sum().sqrt()  # constant variable\n","    b = ((y_pred - yhat_bar) ** 2).sum().sqrt()  # std of pred\n","\n","    a_i = y_true - y_bar\n","    d_i = y_pred - yhat_bar\n","    a = (a_i * d_i).sum()\n","    gradient = c * (a_i / b - a * d_i / b**3)\n","    hessian = - (np.matmul(a_i.reshape(-1, 1), d_i.reshape(1, -1)) + np.matmul(d_i.reshape(-1, 1), a_i.reshape(1, -1))) / b ** 3 + \\\n","              3 * a * np.matmul(d_i.reshape(-1, 1), d_i.reshape(1, -1)) / b**5 + a/(n*b**3)\n","    hessian = hessian - np.ones(shape=(n, n)) * a/b**3\n","    hessian *= c\n","    return -gradient, -hessian"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aY3nu2oCnsAJ"},"source":["train_df = ...\n","era_idx = [np.where(eras == e)[0] for e in np.unique(train_df.eras)]\n","\n","def loss_fn(target, pred):\n","    pred = torch.tensor(pred, requires_grad=True)\n","    target = torch.tensor(target)\n","    corrs = torch.stack([spearman(target[e], pred[e]) for e in era_idx])\n","    sharpe = adj_sharpe(corrs)\n","    gradient = torch.autograd.grad(sharpe, pred)[0].detach().numpy()\n","    hessian = np.ones_like(gradient)  # ones for hessian should be ~okay~\n","    return gradient, hessian\n","\n","model = XGBRegressor(objective=loss_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xFMV_VRMorDT"},"source":["from fast_soft_sort.tf_ops import soft_rank\n","import tensorflow as tf\n","import numpy as np\n","\n","def pearson_corr(x, y):\n","    \n","    xy_t = tf.concat([x, y], axis=0)\n","    mean_t = tf.reduce_mean(xy_t, axis=1, keepdims=True)\n","    cov_t = ((xy_t-mean_t) @ tf.transpose(xy_t-mean_t))/(x.shape[1]-1)\n","    cov2_t = tf.linalg.diag(1/tf.sqrt(tf.linalg.diag_part(cov_t)))\n","    corr_matrix = cov2_t @ cov_t @ cov2_t\n","    corr = tf.reduce_mean(corr_matrix) * 2 - 1 # equivalent to taking element [0][1] assuming the 2x2 corr matrix is symmetric and the diagonals are 1\n","    \n","    return corr\n","\n","def spearman_corr(x, y):\n","    \n","    ranks = soft_rank(x, regularization_strength=0.1)\n","    corr = pearson_corr(ranks, y)\n","    \n","    return corr\n","\n","def get_value_grad_and_hess(x, y, f):\n","    \n","    x_var = tf.Variable(x, dtype=tf.float32)\n","    y_var = tf.Variable(y, dtype=tf.float32)\n","        \n","    val, grad, hess = None, None, None\n","\n","    with tf.GradientTape() as t2:\n","    \n","        with tf.GradientTape() as t1:\n","            \n","            val = f(x_var, y_var)\n","        \n","        grad = t1.gradient(val, x_var)    \n","\n","    hess = t2.jacobian(grad, x_var)\n","\n","    return val, grad, hess\n","\n","# test with random input\n","x = np.random.rand(1, 10) # predictions\n","y = np.random.rand(1, 10) # labels\n","\n","print('pearson:')\n","val, grad, hess = get_value_grad_and_hess(x, y, pearson_corr)\n","print(' value:',  val)\n","print(' gradient:', grad)\n","print(' hessian:', hess)\n","\n","print('spearman:')\n","val, grad, hess = get_value_grad_and_hess(x, y, spearman_corr)\n","print(' value:',  val)\n","print(' gradient:', grad)\n","print(' hessian:', hess)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wjhZTR2K20wm"},"source":["https://github.com/iRyanBell/numerai_kfold_ensemble/blob/master/NumerAI_KFold_Ensemble.ipynb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cgKTsNB921xQ"},"source":["https://github.com/iRyanBell/ARIMA_Stock_Correlations/blob/master/ARIMA_Stock_Correlations.ipynb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oMisJNnf23Fl"},"source":["import numpy as np\n","from scipy.stats import spearmanr\n","\n","def loss_fn(target, pred):\n","    pred = torch.tensor(pred, requires_grad=True)\n","    target = torch.tensor(target)\n","    corrs = torch.stack([spearmanr(target[e], pred[e]) for e in era_idx])\n","    sharpe = adj_sharpe(corrs)\n","    gradient = torch.autograd.grad(sharpe, pred)[0].detach().numpy()\n","    hessian = np.ones_like(gradient)  # ones for hessian should be ~okay~\n","    return gradient, hessian\n","\n","#model = XGBRegressor(objective=loss_fn)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IQSPPcRm3fuF"},"source":["import numpy as np \n","import pandas as pd \n","from xgboost import XGBRegressor \n","import torch\n","from torch.autograd import grad\n","\n","# fit an initial model\n","model_init = XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=2000, colsample_bytree=0.1, nthread=6)\n","model_init.fit(training_data[feature_names], training_data[TARGET_NAME])\n","\n","# get prediction from initial model as starting point to improve upon\n","base_margin = model_init.predict(training_data[feature_names])\n","\n","# get indexes for each era\n","era_idx = [np.where(training_data.era==uera)[0] for uera in training_data.era.unique()]"],"execution_count":null,"outputs":[]}]}