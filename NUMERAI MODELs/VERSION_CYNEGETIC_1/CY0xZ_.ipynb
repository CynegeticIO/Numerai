{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOOFa1Yw1I6gi0Qm9O/WON7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SFhmJ2fjhSIB","executionInfo":{"status":"ok","timestamp":1682856909587,"user_tz":-120,"elapsed":22434,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}},"outputId":"b506bb11-6926-4f83-c898-56db76f131a7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at drive\n"]}]},{"cell_type":"code","metadata":{"id":"Ywcs8DyDvCl4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682856928661,"user_tz":-120,"elapsed":9485,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}},"outputId":"0576be11-7f80-4187-f188-0be23b3ebda5"},"source":["!pip install duckdb halo numerapi"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: duckdb in /usr/local/lib/python3.10/dist-packages (0.7.1)\n","Collecting halo\n","  Downloading halo-0.0.31.tar.gz (11 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting numerapi\n","  Downloading numerapi-2.14.0-py3-none-any.whl (26 kB)\n","Collecting log_symbols>=0.0.14\n","  Downloading log_symbols-0.0.14-py3-none-any.whl (3.1 kB)\n","Collecting spinners>=0.0.24\n","  Downloading spinners-0.0.24-py3-none-any.whl (5.5 kB)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from halo) (2.3.0)\n","Collecting colorama>=0.3.9\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from halo) (1.16.0)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from numerapi) (8.1.3)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from numerapi) (2022.7.1)\n","Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.10/dist-packages (from numerapi) (4.65.0)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from numerapi) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from numerapi) (2.27.1)\n","Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from numerapi) (1.5.3)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->numerapi) (1.22.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->numerapi) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->numerapi) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->numerapi) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->numerapi) (3.4)\n","Building wheels for collected packages: halo\n","  Building wheel for halo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for halo: filename=halo-0.0.31-py3-none-any.whl size=11259 sha256=08f23c4451e3ed8bcda70dd399acd8f7395017f899fb68a980e967934fe97139\n","  Stored in directory: /root/.cache/pip/wheels/5a/d9/8a/b4f14c44aba7c164d4379eca6f1dde59360050406b1edaec24\n","Successfully built halo\n","Installing collected packages: spinners, colorama, log_symbols, numerapi, halo\n","Successfully installed colorama-0.4.6 halo-0.0.31 log_symbols-0.0.14 numerapi-2.14.0 spinners-0.0.24\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import scipy\n","from halo import Halo\n","from tqdm import tqdm\n","from pathlib import Path\n","import json\n","from scipy.stats import skew\n","import numerapi\n","from lightgbm import LGBMRegressor\n","import gc\n","\n","ERA_COL = \"era\"\n","TARGET_COL = \"target_nomi_v4_20\"\n","DATA_TYPE_COL = \"data_type\"\n","EXAMPLE_PREDS_COL = \"example_preds\"\n","\n","spinner = Halo(text=\"\", spinner=\"dots\")\n","\n","MODEL_FOLDER = \"models\"\n","MODEL_CONFIGS_FOLDER = \"model_configs\"\n","PREDICTION_FILES_FOLDER = \"prediction_files\"\n","\n","\n","def save_prediction(df, name):\n","    try:\n","        Path(PREDICTION_FILES_FOLDER).mkdir(exist_ok=True, parents=True)\n","    except Exception as ex:\n","        pass\n","    df.to_csv(f\"{PREDICTION_FILES_FOLDER}/{name}.csv\", index=True)\n","\n","\n","def save_model(model, name):\n","    try:\n","        Path(MODEL_FOLDER).mkdir(exist_ok=True, parents=True)\n","    except Exception as ex:\n","        pass\n","    pd.to_pickle(model, f\"{MODEL_FOLDER}/{name}.pkl\")\n","\n","\n","def load_model(name):\n","    path = Path(f\"{MODEL_FOLDER}/{name}.pkl\")\n","    if path.is_file():\n","        model = pd.read_pickle(f\"{MODEL_FOLDER}/{name}.pkl\")\n","    else:\n","        model = False\n","    return model\n","\n","\n","def save_model_config(model_config, model_name):\n","    try:\n","        Path(MODEL_CONFIGS_FOLDER).mkdir(exist_ok=True, parents=True)\n","    except Exception as ex:\n","        pass\n","    with open(f\"{MODEL_CONFIGS_FOLDER}/{model_name}.json\", \"w\") as fp:\n","        json.dump(model_config, fp)\n","\n","\n","def load_model_config(model_name):\n","    path_str = f\"{MODEL_CONFIGS_FOLDER}/{model_name}.json\"\n","    path = Path(path_str)\n","    if path.is_file():\n","        with open(path_str, \"r\") as fp:\n","            model_config = json.load(fp)\n","    else:\n","        model_config = False\n","    return model_config\n","\n","\n","def get_biggest_change_features(corrs, n):\n","    all_eras = corrs.index.sort_values()\n","    h1_eras = all_eras[: len(all_eras) // 2]\n","    h2_eras = all_eras[len(all_eras) // 2 :]\n","\n","    h1_corr_means = corrs.loc[h1_eras, :].mean()\n","    h2_corr_means = corrs.loc[h2_eras, :].mean()\n","\n","    corr_diffs = h2_corr_means - h1_corr_means\n","    worst_n = corr_diffs.abs().sort_values(ascending=False).head(n).index.tolist()\n","    return worst_n\n","\n","\n","def get_time_series_cross_val_splits(data, cv=3, embargo=12):\n","    all_train_eras = data[ERA_COL].unique()\n","    len_split = len(all_train_eras) // cv\n","    test_splits = [\n","        all_train_eras[i * len_split : (i + 1) * len_split] for i in range(cv)\n","    ]\n","    # fix the last test split to have all the last eras, in case the number of eras wasn't divisible by cv\n","    remainder = len(all_train_eras) % cv\n","    if remainder != 0:\n","        test_splits[-1] = np.append(test_splits[-1], all_train_eras[-remainder:])\n","\n","    train_splits = []\n","    for test_split in test_splits:\n","        test_split_max = int(np.max(test_split))\n","        test_split_min = int(np.min(test_split))\n","        # get all of the eras that aren't in the test split\n","        train_split_not_embargoed = [\n","            e\n","            for e in all_train_eras\n","            if not (test_split_min <= int(e) <= test_split_max)\n","        ]\n","        # embargo the train split so we have no leakage.\n","        # one era is length 5, so we need to embargo by target_length/5 eras.\n","        # To be consistent for all targets, let's embargo everything by 60/5 == 12 eras.\n","        train_split = [\n","            e\n","            for e in train_split_not_embargoed\n","            if abs(int(e) - test_split_max) > embargo\n","            and abs(int(e) - test_split_min) > embargo\n","        ]\n","        train_splits.append(train_split)\n","\n","    # convenient way to iterate over train and test splits\n","    train_test_zip = zip(train_splits, test_splits)\n","    return train_test_zip\n","\n","\n","def neutralize(\n","    df, columns, neutralizers=None, proportion=1.0, normalize=True, era_col=\"era\", verbose=False\n","):\n","    if neutralizers is None:\n","        neutralizers = []\n","    unique_eras = df[era_col].unique()\n","    computed = []\n","    if verbose:\n","        iterator = tqdm(unique_eras)\n","    else:\n","        iterator = unique_eras\n","    for u in iterator:\n","        df_era = df[df[era_col] == u]\n","        scores = df_era[columns].values\n","        if normalize:\n","            scores2 = []\n","            for x in scores.T:\n","                x = (scipy.stats.rankdata(x, method=\"ordinal\") - 0.5) / len(x)\n","                x = scipy.stats.norm.ppf(x)\n","                scores2.append(x)\n","            scores = np.array(scores2).T\n","        exposures = df_era[neutralizers].values\n","\n","        scores -= proportion * exposures.dot(\n","            np.linalg.pinv(exposures.astype(np.float32), rcond=1e-6).dot(\n","                scores.astype(np.float32)\n","            )\n","        )\n","\n","        scores /= scores.std(ddof=0)\n","\n","        computed.append(scores)\n","\n","    return pd.DataFrame(np.concatenate(computed), columns=columns, index=df.index)\n","\n","\n","def neutralize_series(series, by, proportion=1.0):\n","    scores = series.values.reshape(-1, 1)\n","    exposures = by.values.reshape(-1, 1)\n","\n","    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n","    exposures = np.hstack(\n","        (exposures, np.array([np.mean(series)] * len(exposures)).reshape(-1, 1))\n","    )\n","\n","    correction = proportion * (\n","        exposures.dot(np.linalg.lstsq(exposures, scores, rcond=None)[0])\n","    )\n","    corrected_scores = scores - correction\n","    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n","    return neutralized\n","\n","\n","def unif(df):\n","    x = (df.rank(method=\"first\") - 0.5) / len(df)\n","    return pd.Series(x, index=df.index)\n","\n","\n","def get_feature_neutral_mean(\n","    df, prediction_col, target_col, features_for_neutralization=None\n","):\n","    if features_for_neutralization is None:\n","        features_for_neutralization = [c for c in df.columns if c.startswith(\"feature\")]\n","    df.loc[:, \"neutral_sub\"] = neutralize(\n","        df, [prediction_col], features_for_neutralization\n","    )[prediction_col]\n","    scores = (\n","        df.groupby(\"era\")\n","        .apply(lambda x: (unif(x[\"neutral_sub\"]).corr(x[target_col])))\n","        .mean()\n","    )\n","    return np.mean(scores)\n","\n","\n","def get_feature_neutral_mean_tb_era(\n","    df, prediction_col, target_col, tb, features_for_neutralization=None\n","):\n","    if features_for_neutralization is None:\n","        features_for_neutralization = [c for c in df.columns if c.startswith(\"feature\")]\n","    temp_df = df.reset_index(\n","        drop=True\n","    ).copy()  # Reset index due to use of argsort later\n","    temp_df.loc[:, \"neutral_sub\"] = neutralize(\n","        temp_df, [prediction_col], features_for_neutralization\n","    )[prediction_col]\n","    temp_df_argsort = temp_df.loc[:, \"neutral_sub\"].argsort()\n","    temp_df_tb_idx = pd.concat([temp_df_argsort.iloc[:tb], temp_df_argsort.iloc[-tb:]])\n","    temp_df_tb = temp_df.loc[temp_df_tb_idx]\n","    tb_fnc = unif(temp_df_tb[\"neutral_sub\"]).corr(temp_df_tb[target_col])\n","    return tb_fnc\n","\n","\n","def fast_score_by_date(df, columns, target, tb=None, era_col=\"era\"):\n","    unique_eras = df[era_col].unique()\n","    computed = []\n","    for u in unique_eras:\n","        df_era = df[df[era_col] == u]\n","        era_pred = np.float64(df_era[columns].values.T)\n","        era_target = np.float64(df_era[target].values.T)\n","\n","        if tb is None:\n","            ccs = np.corrcoef(era_target, era_pred)[0, 1:]\n","        else:\n","            tbidx = np.argsort(era_pred, axis=1)\n","            tbidx = np.concatenate([tbidx[:, :tb], tbidx[:, -tb:]], axis=1)\n","            ccs = [\n","                np.corrcoef(era_target[tmpidx], tmppred[tmpidx])[0, 1]\n","                for tmpidx, tmppred in zip(tbidx, era_pred)\n","            ]\n","            ccs = np.array(ccs)\n","\n","        computed.append(ccs)\n","\n","    return pd.DataFrame(np.array(computed), columns=columns, index=df[era_col].unique())\n","\n","\n","def exposure_dissimilarity_per_era(df, prediction_col, example_col, feature_cols=None):\n","    if feature_cols is None:\n","        feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n","    u = df.loc[:, feature_cols].corrwith(df[prediction_col])\n","    e = df.loc[:, feature_cols].corrwith(df[example_col])\n","    return 1 - (np.dot(u, e) / np.dot(e, e))\n","\n","\n","def validation_metrics(\n","    validation_data,\n","    pred_cols,\n","    example_col,\n","    fast_mode=False,\n","    target_col=TARGET_COL,\n","    features_for_neutralization=None,\n","):\n","    validation_stats = pd.DataFrame()\n","    feature_cols = [c for c in validation_data if c.startswith(\"feature_\")]\n","    for pred_col in pred_cols:\n","        # Check the per-era correlations on the validation set (out of sample)\n","        validation_correlations = validation_data.groupby(ERA_COL).apply(\n","            lambda d: unif(d[pred_col]).corr(d[target_col])\n","        )\n","\n","        mean = validation_correlations.mean()\n","        std = validation_correlations.std(ddof=0)\n","        sharpe = mean / std\n","\n","        validation_stats.loc[\"mean\", pred_col] = mean\n","        validation_stats.loc[\"std\", pred_col] = std\n","        validation_stats.loc[\"sharpe\", pred_col] = sharpe\n","\n","        rolling_max = (\n","            (validation_correlations + 1)\n","            .cumprod()\n","            .rolling(window=9000, min_periods=1)  # arbitrarily large\n","            .max()\n","        )\n","        daily_value = (validation_correlations + 1).cumprod()\n","        max_drawdown = -((rolling_max - daily_value) / rolling_max).max()\n","        validation_stats.loc[\"max_drawdown\", pred_col] = max_drawdown\n","\n","        payout_scores = validation_correlations.clip(-0.25, 0.25)\n","        payout_daily_value = (payout_scores + 1).cumprod()\n","\n","        apy = (\n","            ((payout_daily_value.dropna().iloc[-1]) ** (1 / len(payout_scores)))\n","            ** 49  # 52 weeks of compounding minus 3 for stake compounding lag\n","            - 1\n","        ) * 100\n","\n","        validation_stats.loc[\"apy\", pred_col] = apy\n","\n","        if not fast_mode:\n","            # Check the feature exposure of your validation predictions\n","            max_per_era = validation_data.groupby(ERA_COL).apply(\n","                lambda d: d[feature_cols].corrwith(d[pred_col]).abs().max()\n","            )\n","            max_feature_exposure = max_per_era.mean()\n","            validation_stats.loc[\n","                \"max_feature_exposure\", pred_col\n","            ] = max_feature_exposure\n","\n","            # Check feature neutral mean\n","            feature_neutral_mean = get_feature_neutral_mean(\n","                validation_data, pred_col, target_col, features_for_neutralization\n","            )\n","            validation_stats.loc[\n","                \"feature_neutral_mean\", pred_col\n","            ] = feature_neutral_mean\n","\n","            # Check TB200 feature neutral mean\n","            tb200_feature_neutral_mean_era = validation_data.groupby(ERA_COL).apply(\n","                lambda df: get_feature_neutral_mean_tb_era(\n","                    df, pred_col, target_col, 200, features_for_neutralization\n","                )\n","            )\n","            validation_stats.loc[\n","                \"tb200_feature_neutral_mean\", pred_col\n","            ] = tb200_feature_neutral_mean_era.mean()\n","\n","            # Check top and bottom 200 metrics (TB200)\n","            tb200_validation_correlations = fast_score_by_date(\n","                validation_data, [pred_col], target_col, tb=200, era_col=ERA_COL\n","            )\n","\n","            tb200_mean = tb200_validation_correlations.mean()[pred_col]\n","            tb200_std = tb200_validation_correlations.std(ddof=0)[pred_col]\n","            tb200_sharpe = tb200_mean / tb200_std\n","\n","            validation_stats.loc[\"tb200_mean\", pred_col] = tb200_mean\n","            validation_stats.loc[\"tb200_std\", pred_col] = tb200_std\n","            validation_stats.loc[\"tb200_sharpe\", pred_col] = tb200_sharpe\n","\n","        # MMC over validation\n","        mmc_scores = []\n","        corr_scores = []\n","        for _, x in validation_data.groupby(ERA_COL):\n","            series = neutralize_series(unif(x[pred_col]), (x[example_col]))\n","            mmc_scores.append(np.cov(series, x[target_col])[0, 1] / (0.29**2))\n","            corr_scores.append(unif(x[pred_col]).corr(x[target_col]))\n","\n","        val_mmc_mean = np.mean(mmc_scores)\n","        val_mmc_std = np.std(mmc_scores)\n","        corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n","        corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n","\n","        validation_stats.loc[\"mmc_mean\", pred_col] = val_mmc_mean\n","        validation_stats.loc[\"corr_plus_mmc_sharpe\", pred_col] = corr_plus_mmc_sharpe\n","\n","        # Check correlation with example predictions\n","        per_era_corrs = validation_data.groupby(ERA_COL).apply(\n","            lambda d: unif(d[pred_col]).corr(unif(d[example_col]))\n","        )\n","        corr_with_example_preds = per_era_corrs.mean()\n","        validation_stats.loc[\n","            \"corr_with_example_preds\", pred_col\n","        ] = corr_with_example_preds\n","\n","        # Check exposure dissimilarity per era\n","        tdf = validation_data.groupby(ERA_COL).apply(\n","            lambda df: exposure_dissimilarity_per_era(\n","                df, pred_col, example_col, feature_cols\n","            )\n","        )\n","        validation_stats.loc[\"exposure_dissimilarity_mean\", pred_col] = tdf.mean()\n","\n","    # .transpose so that stats are columns and the model_name is the row\n","    return validation_stats.transpose()"],"metadata":{"id":"7Rtz8hjjpgP4","executionInfo":{"status":"ok","timestamp":1682856931763,"user_tz":-120,"elapsed":3107,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gfJq2lgOZpJd","executionInfo":{"status":"ok","timestamp":1682856931764,"user_tz":-120,"elapsed":11,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"id":"O8N-WAmxs2qX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682856931764,"user_tz":-120,"elapsed":11,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}},"outputId":"15fcb8eb-5eb3-4a6e-b615-67c19d8fa27a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Apr 30 12:15:31 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   31C    P0    45W / 400W |      0MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"id":"Pz-q5UgJs4V_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682856931764,"user_tz":-120,"elapsed":7,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}},"outputId":"0a0c1ff4-3361-420c-8337-bc9f03e61ade"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 89.6 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}]},{"cell_type":"code","metadata":{"id":"HEg-KrxVs981","executionInfo":{"status":"ok","timestamp":1682856931764,"user_tz":-120,"elapsed":5,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}}},"source":["class PandasDriver:\n","    def __init__(self, pq_path: str, splits=4):\n","        self.pq_path = pq_path\n","        self.splits = splits\n","        \n","        self.df = pd.read_parquet(pq_path)\n","        self.df['era'] = self.df['era'].astype('int')\n","    \n","    def get_by_group(self, group_id: int, cols=None):\n","        if group_id == self.splits:\n","            group_id = 0\n","        return self.df[self.df['era'] % self.splits == group_id]\n","\n","\n","import duckdb\n","\n","class DuckDBDriver:\n","    def __init__(self, pq_path: str, splits=4):\n","        self.pq_path = pq_path\n","        self.splits = splits\n","        self.conn = duckdb.connect(\":memory:\")\n","        \n","    def _gen_select_statement(self) -> str:\n","        return f\"SELECT * FROM parquet_scan('{self.pq_path}') \"\n","    \n","    def _query(self, expression: str):\n","        return self. conn.execute(expression)\n","    \n","    def _fetch(self, ret_query, fetch_type, cols):\n","        if fetch_type == \"pandas\":\n","            return ret_query.fetchdf()\n","        elif fetch_type == \"numpy\":\n","            return ret_query.fetchdf()[cols].values\n","        \n","    def get_by_era(self, era: str, cols=None, fetch_type=\"pandas\"):\n","        expression = self._gen_select_statement()\n","        expression += f\"WHERE era = '{era}'\"\n","        ret_query = self._query(expression)\n","        return self._fetch(ret_query, fetch_type, cols)\n","    \n","    def get_by_group(self, group_id: int, cols=None, fetch_type=\"pandas\"):\n","        if group_id == self.splits:\n","            group_id = 0\n","        expression = self._gen_select_statement()\n","        expression += f\"WHERE CAST(era AS INT) % {self.splits} = {group_id}\"\n","        ret_query = self._query(expression)\n","        return self._fetch(ret_query, fetch_type, cols)"],"execution_count":6,"outputs":[]},{"cell_type":"code","source":["###########################################################################################\n","################################   CY0xZ   ################################################\n","###########################################################################################"],"metadata":{"id":"9xvh5DVOSfRY","executionInfo":{"status":"ok","timestamp":1682856931765,"user_tz":-120,"elapsed":5,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import gc"],"metadata":{"id":"w3NL5TGdBAqi","executionInfo":{"status":"ok","timestamp":1682856931765,"user_tz":-120,"elapsed":5,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["napi = numerapi.NumerAPI()\n","\n","current_round = napi.get_current_round()"],"metadata":{"id":"N65jo6Te_xGS","executionInfo":{"status":"ok","timestamp":1682856932173,"user_tz":-120,"elapsed":413,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["current_round"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"huES4iKU5gvx","executionInfo":{"status":"ok","timestamp":1682856932174,"user_tz":-120,"elapsed":10,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}},"outputId":"78fd13bb-0276-4e95-aa04-635a2a300ece"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["474"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["print('Downloading dataset files...')\n","\n","Path(\"./v4\").mkdir(parents=False, exist_ok=True)\n","napi.download_dataset(\"v4/train.parquet\")\n","napi.download_dataset(\"v4/validation.parquet\")\n","napi.download_dataset(\"v4/live.parquet\", f\"v4/live_{current_round}.parquet\")\n","napi.download_dataset(\"v4/validation_example_preds.parquet\")\n","napi.download_dataset(\"v4/features.json\")\n","\n","print('Reading minimal training data')\n","# read the feature metadata and get a feature set (or all the features)\n","with open(\"v4/features.json\", \"r\") as f:\n","    feature_metadata = json.load(f)\n","# features = list(feature_metadata[\"feature_stats\"].keys()) # get all the features\n","# features = feature_metadata[\"feature_sets\"][\"small\"] # get the small feature set\n","features = feature_metadata[\"feature_sets\"][\"medium\"] # get the medium feature set\n","# read in just those features along with era and target columns\n","read_columns = features + [ERA_COL, DATA_TYPE_COL, TARGET_COL]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sP_0kYA__vNH","executionInfo":{"status":"ok","timestamp":1682857087503,"user_tz":-120,"elapsed":155335,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}},"outputId":"09bf848a-f899-43cd-9b43-0ccb126de50e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading dataset files...\n"]},{"output_type":"stream","name":"stderr","text":["v4/train.parquet: 1.16GB [01:13, 15.8MB/s]                            \n","v4/validation.parquet: 1.19GB [01:08, 17.3MB/s]                            \n","v4/live_474.parquet: 3.39MB [00:01, 2.82MB/s]                            \n","v4/validation_example_preds.parquet: 58.5MB [00:04, 12.4MB/s]                            \n","v4/features.json: 562kB [00:00, 832kB/s]                           \n"]},{"output_type":"stream","name":"stdout","text":["Reading minimal training data\n"]}]},{"cell_type":"code","source":["training_data = pd.read_parquet('v4/train.parquet', columns=read_columns)\n","validation_data = pd.read_parquet('v4/validation.parquet', columns=read_columns)\n","live_data = pd.read_parquet(f'v4/live_{current_round}.parquet', columns=read_columns)"],"metadata":{"id":"z8OZrd8f_hEw","executionInfo":{"status":"ok","timestamp":1682857093009,"user_tz":-120,"elapsed":5515,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# getting the per era correlation of each feature vs the target\n","all_feature_corrs = training_data.groupby(ERA_COL).apply(lambda era: era[features].corrwith(era[TARGET_COL]))\n","# find the riskiest features by comparing their correlation vs\n","# the target in each half of training data; we'll use these later\n","riskiest_features = get_biggest_change_features(all_feature_corrs, 350)\n","# \"garbage collection\" (gc) gets rid of unused data and frees up memory\n","gc.collect()"],"metadata":{"id":"ajJ34D7e_hHx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682857182054,"user_tz":-120,"elapsed":89054,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}},"outputId":"a37dddd2-c86f-410d-8c0a-2494f06b2104"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["%%time\n","model_name = f\"model_target\"\n","print(f\"Checking for existing model '{model_name}'\")\n","model = load_model(model_name)\n","if not model:\n","    print(f\"model not found, creating new one\")\n","    params = {\"n_estimators\": 2000,\n","              \"learning_rate\": 0.01,\n","              \"max_depth\": 5,\n","              \"num_leaves\": 2 ** 5,\n","              \"colsample_bytree\": 0.1}\n","\n","    model = LGBMRegressor(**params)\n","\n","    # train on all of train and save the model so we don't have to train next time\n","    model.fit(training_data.filter(like='feature_', axis='columns'), training_data[TARGET_COL]) \n","    print(f\"saving new model: {model_name}\")\n","    save_model(model, model_name)\n","\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MUx-ynSh_hKI","executionInfo":{"status":"ok","timestamp":1682857554020,"user_tz":-120,"elapsed":371985,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}},"outputId":"0d980fe7-59af-4e3a-bbec-1608d9e9d330"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Checking for existing model 'model_target'\n","model not found, creating new one\n","saving new model: model_target\n","CPU times: user 1h 9min 28s, sys: 5.17 s, total: 1h 9min 33s\n","Wall time: 6min 11s\n"]},{"output_type":"execute_result","data":{"text/plain":["41"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["nans_per_col = live_data[live_data[\"data_type\"] == \"live\"][features].isna().sum()"],"metadata":{"id":"MEossB6K_hM0","executionInfo":{"status":"ok","timestamp":1682857554021,"user_tz":-120,"elapsed":16,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# check for nans and fill nans\n","if nans_per_col.any():\n","    total_rows = len(live_data[live_data[\"data_type\"] == \"live\"])\n","    print(f\"Number of nans per column this week: {nans_per_col[nans_per_col > 0]}\")\n","    print(f\"out of {total_rows} total rows\")\n","    print(f\"filling nans with 0.5\")\n","    live_data.loc[:, features] = live_data.loc[:, features].fillna(0.5)\n","\n","else:\n","    print(\"No nans in the features this week!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kMmlto-BCPp7","executionInfo":{"status":"ok","timestamp":1682857554021,"user_tz":-120,"elapsed":15,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}},"outputId":"d1e27afc-02e7-4fca-d820-bfea075fdbe3"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["No nans in the features this week!\n"]}]},{"cell_type":"code","source":["%%time\n","# double check the feature that the model expects vs what is available to prevent our\n","# pipeline from failing if Numerai adds more data and we don't have time to retrain!\n","model_expected_features = model.booster_.feature_name()\n","\n","if set(model_expected_features) != set(features):\n","    print(f\"New features are available! Might want to retrain model {model_name}.\")\n","\n","validation_data.loc[:, f\"preds_{model_name}\"] = model.predict(validation_data.loc[:, model_expected_features])\n","\n","live_data.loc[:, f\"preds_{model_name}\"] = model.predict(live_data.loc[:, model_expected_features])\n","\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XKHqWPnHAsaC","executionInfo":{"status":"ok","timestamp":1682857620212,"user_tz":-120,"elapsed":66204,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}},"outputId":"499b439a-6699-4734-b052-455be78b286e"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 11min 19s, sys: 2.5 s, total: 11min 22s\n","Wall time: 1min 5s\n"]},{"output_type":"execute_result","data":{"text/plain":["23"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["%%time\n","# neutralize our predictions to the riskiest features\n","validation_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(\n","    df=validation_data,\n","    columns=[f\"preds_{model_name}\"],\n","    neutralizers=riskiest_features,\n","    proportion=1.0,\n","    normalize=True,\n","    era_col=ERA_COL\n",")\n","\n","live_data[f\"preds_{model_name}_neutral_riskiest_50\"] = neutralize(\n","    df=live_data,\n","    columns=[f\"preds_{model_name}\"],\n","    neutralizers=riskiest_features,\n","    proportion=1.0,\n","    normalize=True,\n","    era_col=ERA_COL\n",")\n","\n","model_to_submit = f\"preds_{model_name}_neutral_riskiest_50\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W0NUTjrRAsdP","executionInfo":{"status":"ok","timestamp":1682857795524,"user_tz":-120,"elapsed":175321,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}},"outputId":"f7b8ab68-0e61-4615-fedc-ad7090e888f7"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 8min 41s, sys: 15min 23s, total: 24min 5s\n","Wall time: 2min 55s\n"]}]},{"cell_type":"code","source":["# rename best model to \"prediction\" and rank from 0 to 1 to meet upload requirements\n","validation_data[\"prediction\"] = validation_data[model_to_submit].rank(pct=True)\n","live_data[\"prediction\"] = live_data[model_to_submit].rank(pct=True)\n","validation_data[\"prediction\"].to_csv(f\"validation_predictions_{current_round}.csv\")\n","live_data[\"prediction\"].to_csv(f\"live_predictions_{current_round}.csv\")\n","\n","validation_preds = pd.read_parquet('v4/validation_example_preds.parquet')\n","validation_data[EXAMPLE_PREDS_COL] = validation_preds[\"prediction\"]"],"metadata":{"id":"UTKkwLzjAxot","executionInfo":{"status":"ok","timestamp":1682857801943,"user_tz":-120,"elapsed":6429,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["f\"live_predictions_{current_round}.csv\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"uuZ4Aup4DtN8","executionInfo":{"status":"ok","timestamp":1682857801943,"user_tz":-120,"elapsed":27,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}},"outputId":"c7b77076-b224-4e94-fb97-c074ab8f33f0"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'live_predictions_474.csv'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["!cp live_predictions_474.csv \"drive/My Drive/Cynegetic Investment Management/\""],"metadata":{"id":"hXQqWJSYDhYX","executionInfo":{"status":"ok","timestamp":1682858230033,"user_tz":-120,"elapsed":420,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["f\"validation_predictions_{current_round}.csv\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"zoq7kiXPEAUN","executionInfo":{"status":"ok","timestamp":1682858230889,"user_tz":-120,"elapsed":2,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}},"outputId":"20bef3db-f445-490c-e040-4e7d059cef43"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'validation_predictions_474.csv'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["!cp validation_predictions_474.csv \"drive/My Drive/Cynegetic Investment Management/\""],"metadata":{"id":"aylXoCjREFn3","executionInfo":{"status":"ok","timestamp":1682858235016,"user_tz":-120,"elapsed":491,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["%%time\n","# get some stats about each of our models to compare...\n","# fast_mode=True so that we skip some of the stats that are slower to calculate\n","validation_stats = validation_metrics(validation_data, [model_to_submit, f\"preds_{model_name}\"], example_col=EXAMPLE_PREDS_COL, fast_mode=True, target_col=TARGET_COL)\n","print(validation_stats[[\"mean\", \"sharpe\"]].to_markdown())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k4Qp0AHfAseq","executionInfo":{"status":"ok","timestamp":1682266543564,"user_tz":-120,"elapsed":334035,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}},"outputId":"bd26c835-7715-4390-8b0c-f6c947e5fa08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["|                                        |      mean |   sharpe |\n","|:---------------------------------------|----------:|---------:|\n","| preds_model_target_neutral_riskiest_50 | 0.0222454 | 1.07558  |\n","| preds_model_target                     | 0.0266606 | 0.850939 |\n","CPU times: user 5min 33s, sys: 54.9 s, total: 6min 28s\n","Wall time: 5min 34s\n"]}]},{"cell_type":"code","source":["print(f'''\n","Done! Next steps:\n","    1. Go to numer.ai/tournament (make sure you have an account)\n","    2. Submit validation_predictions_{current_round}.csv to the diagnostics tool\n","    3. Submit tournament_predictions_{current_round}.csv to the \"Upload Predictions\" button\n","''')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uIuzU9gL_hOW","executionInfo":{"status":"ok","timestamp":1681637380856,"user_tz":-120,"elapsed":221,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}},"outputId":"193b9695-172b-4c00-c70e-3b307240019d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Done! Next steps:\n","    1. Go to numer.ai/tournament (make sure you have an account)\n","    2. Submit validation_predictions_464.csv to the diagnostics tool\n","    3. Submit tournament_predictions_464.csv to the \"Upload Predictions\" button\n","\n"]}]},{"cell_type":"code","source":["################################################################################\n","##################   FINAL EMERGENCI SUBMISSION   ##############################\n","################################################################################"],"metadata":{"id":"7ZGv2Zj9rhra"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RnpjlVdG1JZT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"FPDw1SQR1MT5"},"execution_count":null,"outputs":[]}]}