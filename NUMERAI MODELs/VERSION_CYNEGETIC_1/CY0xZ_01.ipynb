{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMCz2cTyQzHnxFcSOMTiJeq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SFhmJ2fjhSIB","executionInfo":{"status":"ok","timestamp":1681635501781,"user_tz":-120,"elapsed":15294,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}},"outputId":"c492391f-27d8-4d7c-8d04-d927ce8e8896"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at drive\n"]}]},{"cell_type":"code","metadata":{"id":"Ywcs8DyDvCl4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681635641768,"user_tz":-120,"elapsed":3830,"user":{"displayName":"Joan-Marc Fisa Gol","userId":"00374894131735723747"}},"outputId":"3d8ecd76-74fa-459d-d81b-7d58bdd80f20"},"source":["!pip install duckdb halo numerapi"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: duckdb in /usr/local/lib/python3.9/dist-packages (0.7.1)\n","Requirement already satisfied: halo in /usr/local/lib/python3.9/dist-packages (0.0.31)\n","Collecting numerapi\n","  Downloading numerapi-2.14.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: colorama>=0.3.9 in /usr/local/lib/python3.9/dist-packages (from halo) (0.4.6)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from halo) (2.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from halo) (1.16.0)\n","Requirement already satisfied: log-symbols>=0.0.14 in /usr/local/lib/python3.9/dist-packages (from halo) (0.0.14)\n","Requirement already satisfied: spinners>=0.0.24 in /usr/local/lib/python3.9/dist-packages (from halo) (0.0.24)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.9/dist-packages (from numerapi) (2022.7.1)\n","Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.9/dist-packages (from numerapi) (4.65.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from numerapi) (2.27.1)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from numerapi) (2.8.2)\n","Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from numerapi) (1.5.3)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from numerapi) (8.1.3)\n","Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.0->numerapi) (1.22.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->numerapi) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->numerapi) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->numerapi) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->numerapi) (2.0.12)\n","Installing collected packages: numerapi\n","Successfully installed numerapi-2.14.0\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import scipy\n","from halo import Halo\n","from tqdm import tqdm\n","from pathlib import Path\n","import json\n","from scipy.stats import skew\n","import numerapi\n","from lightgbm import LGBMRegressor\n","import gc\n","\n","ERA_COL = \"era\"\n","TARGET_COL = \"target_nomi_v4_20\"\n","DATA_TYPE_COL = \"data_type\"\n","EXAMPLE_PREDS_COL = \"example_preds\"\n","\n","spinner = Halo(text=\"\", spinner=\"dots\")\n","\n","MODEL_FOLDER = \"models\"\n","MODEL_CONFIGS_FOLDER = \"model_configs\"\n","PREDICTION_FILES_FOLDER = \"prediction_files\"\n","\n","\n","def save_prediction(df, name):\n","    try:\n","        Path(PREDICTION_FILES_FOLDER).mkdir(exist_ok=True, parents=True)\n","    except Exception as ex:\n","        pass\n","    df.to_csv(f\"{PREDICTION_FILES_FOLDER}/{name}.csv\", index=True)\n","\n","\n","def save_model(model, name):\n","    try:\n","        Path(MODEL_FOLDER).mkdir(exist_ok=True, parents=True)\n","    except Exception as ex:\n","        pass\n","    pd.to_pickle(model, f\"{MODEL_FOLDER}/{name}.pkl\")\n","\n","\n","def load_model(name):\n","    path = Path(f\"{MODEL_FOLDER}/{name}.pkl\")\n","    if path.is_file():\n","        model = pd.read_pickle(f\"{MODEL_FOLDER}/{name}.pkl\")\n","    else:\n","        model = False\n","    return model\n","\n","\n","def save_model_config(model_config, model_name):\n","    try:\n","        Path(MODEL_CONFIGS_FOLDER).mkdir(exist_ok=True, parents=True)\n","    except Exception as ex:\n","        pass\n","    with open(f\"{MODEL_CONFIGS_FOLDER}/{model_name}.json\", \"w\") as fp:\n","        json.dump(model_config, fp)\n","\n","\n","def load_model_config(model_name):\n","    path_str = f\"{MODEL_CONFIGS_FOLDER}/{model_name}.json\"\n","    path = Path(path_str)\n","    if path.is_file():\n","        with open(path_str, \"r\") as fp:\n","            model_config = json.load(fp)\n","    else:\n","        model_config = False\n","    return model_config\n","\n","\n","def get_biggest_change_features(corrs, n):\n","    all_eras = corrs.index.sort_values()\n","    h1_eras = all_eras[: len(all_eras) // 2]\n","    h2_eras = all_eras[len(all_eras) // 2 :]\n","\n","    h1_corr_means = corrs.loc[h1_eras, :].mean()\n","    h2_corr_means = corrs.loc[h2_eras, :].mean()\n","\n","    corr_diffs = h2_corr_means - h1_corr_means\n","    worst_n = corr_diffs.abs().sort_values(ascending=False).head(n).index.tolist()\n","    return worst_n\n","\n","\n","def get_time_series_cross_val_splits(data, cv=3, embargo=12):\n","    all_train_eras = data[ERA_COL].unique()\n","    len_split = len(all_train_eras) // cv\n","    test_splits = [\n","        all_train_eras[i * len_split : (i + 1) * len_split] for i in range(cv)\n","    ]\n","    # fix the last test split to have all the last eras, in case the number of eras wasn't divisible by cv\n","    remainder = len(all_train_eras) % cv\n","    if remainder != 0:\n","        test_splits[-1] = np.append(test_splits[-1], all_train_eras[-remainder:])\n","\n","    train_splits = []\n","    for test_split in test_splits:\n","        test_split_max = int(np.max(test_split))\n","        test_split_min = int(np.min(test_split))\n","        # get all of the eras that aren't in the test split\n","        train_split_not_embargoed = [\n","            e\n","            for e in all_train_eras\n","            if not (test_split_min <= int(e) <= test_split_max)\n","        ]\n","        # embargo the train split so we have no leakage.\n","        # one era is length 5, so we need to embargo by target_length/5 eras.\n","        # To be consistent for all targets, let's embargo everything by 60/5 == 12 eras.\n","        train_split = [\n","            e\n","            for e in train_split_not_embargoed\n","            if abs(int(e) - test_split_max) > embargo\n","            and abs(int(e) - test_split_min) > embargo\n","        ]\n","        train_splits.append(train_split)\n","\n","    # convenient way to iterate over train and test splits\n","    train_test_zip = zip(train_splits, test_splits)\n","    return train_test_zip\n","\n","\n","def neutralize(\n","    df, columns, neutralizers=None, proportion=1.0, normalize=True, era_col=\"era\", verbose=False\n","):\n","    if neutralizers is None:\n","        neutralizers = []\n","    unique_eras = df[era_col].unique()\n","    computed = []\n","    if verbose:\n","        iterator = tqdm(unique_eras)\n","    else:\n","        iterator = unique_eras\n","    for u in iterator:\n","        df_era = df[df[era_col] == u]\n","        scores = df_era[columns].values\n","        if normalize:\n","            scores2 = []\n","            for x in scores.T:\n","                x = (scipy.stats.rankdata(x, method=\"ordinal\") - 0.5) / len(x)\n","                x = scipy.stats.norm.ppf(x)\n","                scores2.append(x)\n","            scores = np.array(scores2).T\n","        exposures = df_era[neutralizers].values\n","\n","        scores -= proportion * exposures.dot(\n","            np.linalg.pinv(exposures.astype(np.float32), rcond=1e-6).dot(\n","                scores.astype(np.float32)\n","            )\n","        )\n","\n","        scores /= scores.std(ddof=0)\n","\n","        computed.append(scores)\n","\n","    return pd.DataFrame(np.concatenate(computed), columns=columns, index=df.index)\n","\n","\n","def neutralize_series(series, by, proportion=1.0):\n","    scores = series.values.reshape(-1, 1)\n","    exposures = by.values.reshape(-1, 1)\n","\n","    # this line makes series neutral to a constant column so that it's centered and for sure gets corr 0 with exposures\n","    exposures = np.hstack(\n","        (exposures, np.array([np.mean(series)] * len(exposures)).reshape(-1, 1))\n","    )\n","\n","    correction = proportion * (\n","        exposures.dot(np.linalg.lstsq(exposures, scores, rcond=None)[0])\n","    )\n","    corrected_scores = scores - correction\n","    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n","    return neutralized\n","\n","\n","def unif(df):\n","    x = (df.rank(method=\"first\") - 0.5) / len(df)\n","    return pd.Series(x, index=df.index)\n","\n","\n","def get_feature_neutral_mean(\n","    df, prediction_col, target_col, features_for_neutralization=None\n","):\n","    if features_for_neutralization is None:\n","        features_for_neutralization = [c for c in df.columns if c.startswith(\"feature\")]\n","    df.loc[:, \"neutral_sub\"] = neutralize(\n","        df, [prediction_col], features_for_neutralization\n","    )[prediction_col]\n","    scores = (\n","        df.groupby(\"era\")\n","        .apply(lambda x: (unif(x[\"neutral_sub\"]).corr(x[target_col])))\n","        .mean()\n","    )\n","    return np.mean(scores)\n","\n","\n","def get_feature_neutral_mean_tb_era(\n","    df, prediction_col, target_col, tb, features_for_neutralization=None\n","):\n","    if features_for_neutralization is None:\n","        features_for_neutralization = [c for c in df.columns if c.startswith(\"feature\")]\n","    temp_df = df.reset_index(\n","        drop=True\n","    ).copy()  # Reset index due to use of argsort later\n","    temp_df.loc[:, \"neutral_sub\"] = neutralize(\n","        temp_df, [prediction_col], features_for_neutralization\n","    )[prediction_col]\n","    temp_df_argsort = temp_df.loc[:, \"neutral_sub\"].argsort()\n","    temp_df_tb_idx = pd.concat([temp_df_argsort.iloc[:tb], temp_df_argsort.iloc[-tb:]])\n","    temp_df_tb = temp_df.loc[temp_df_tb_idx]\n","    tb_fnc = unif(temp_df_tb[\"neutral_sub\"]).corr(temp_df_tb[target_col])\n","    return tb_fnc\n","\n","\n","def fast_score_by_date(df, columns, target, tb=None, era_col=\"era\"):\n","    unique_eras = df[era_col].unique()\n","    computed = []\n","    for u in unique_eras:\n","        df_era = df[df[era_col] == u]\n","        era_pred = np.float64(df_era[columns].values.T)\n","        era_target = np.float64(df_era[target].values.T)\n","\n","        if tb is None:\n","            ccs = np.corrcoef(era_target, era_pred)[0, 1:]\n","        else:\n","            tbidx = np.argsort(era_pred, axis=1)\n","            tbidx = np.concatenate([tbidx[:, :tb], tbidx[:, -tb:]], axis=1)\n","            ccs = [\n","                np.corrcoef(era_target[tmpidx], tmppred[tmpidx])[0, 1]\n","                for tmpidx, tmppred in zip(tbidx, era_pred)\n","            ]\n","            ccs = np.array(ccs)\n","\n","        computed.append(ccs)\n","\n","    return pd.DataFrame(np.array(computed), columns=columns, index=df[era_col].unique())\n","\n","\n","def exposure_dissimilarity_per_era(df, prediction_col, example_col, feature_cols=None):\n","    if feature_cols is None:\n","        feature_cols = [c for c in df.columns if c.startswith(\"feature\")]\n","    u = df.loc[:, feature_cols].corrwith(df[prediction_col])\n","    e = df.loc[:, feature_cols].corrwith(df[example_col])\n","    return 1 - (np.dot(u, e) / np.dot(e, e))\n","\n","\n","def validation_metrics(\n","    validation_data,\n","    pred_cols,\n","    example_col,\n","    fast_mode=False,\n","    target_col=TARGET_COL,\n","    features_for_neutralization=None,\n","):\n","    validation_stats = pd.DataFrame()\n","    feature_cols = [c for c in validation_data if c.startswith(\"feature_\")]\n","    for pred_col in pred_cols:\n","        # Check the per-era correlations on the validation set (out of sample)\n","        validation_correlations = validation_data.groupby(ERA_COL).apply(\n","            lambda d: unif(d[pred_col]).corr(d[target_col])\n","        )\n","\n","        mean = validation_correlations.mean()\n","        std = validation_correlations.std(ddof=0)\n","        sharpe = mean / std\n","\n","        validation_stats.loc[\"mean\", pred_col] = mean\n","        validation_stats.loc[\"std\", pred_col] = std\n","        validation_stats.loc[\"sharpe\", pred_col] = sharpe\n","\n","        rolling_max = (\n","            (validation_correlations + 1)\n","            .cumprod()\n","            .rolling(window=9000, min_periods=1)  # arbitrarily large\n","            .max()\n","        )\n","        daily_value = (validation_correlations + 1).cumprod()\n","        max_drawdown = -((rolling_max - daily_value) / rolling_max).max()\n","        validation_stats.loc[\"max_drawdown\", pred_col] = max_drawdown\n","\n","        payout_scores = validation_correlations.clip(-0.25, 0.25)\n","        payout_daily_value = (payout_scores + 1).cumprod()\n","\n","        apy = (\n","            ((payout_daily_value.dropna().iloc[-1]) ** (1 / len(payout_scores)))\n","            ** 49  # 52 weeks of compounding minus 3 for stake compounding lag\n","            - 1\n","        ) * 100\n","\n","        validation_stats.loc[\"apy\", pred_col] = apy\n","\n","        if not fast_mode:\n","            # Check the feature exposure of your validation predictions\n","            max_per_era = validation_data.groupby(ERA_COL).apply(\n","                lambda d: d[feature_cols].corrwith(d[pred_col]).abs().max()\n","            )\n","            max_feature_exposure = max_per_era.mean()\n","            validation_stats.loc[\n","                \"max_feature_exposure\", pred_col\n","            ] = max_feature_exposure\n","\n","            # Check feature neutral mean\n","            feature_neutral_mean = get_feature_neutral_mean(\n","                validation_data, pred_col, target_col, features_for_neutralization\n","            )\n","            validation_stats.loc[\n","                \"feature_neutral_mean\", pred_col\n","            ] = feature_neutral_mean\n","\n","            # Check TB200 feature neutral mean\n","            tb200_feature_neutral_mean_era = validation_data.groupby(ERA_COL).apply(\n","                lambda df: get_feature_neutral_mean_tb_era(\n","                    df, pred_col, target_col, 200, features_for_neutralization\n","                )\n","            )\n","            validation_stats.loc[\n","                \"tb200_feature_neutral_mean\", pred_col\n","            ] = tb200_feature_neutral_mean_era.mean()\n","\n","            # Check top and bottom 200 metrics (TB200)\n","            tb200_validation_correlations = fast_score_by_date(\n","                validation_data, [pred_col], target_col, tb=200, era_col=ERA_COL\n","            )\n","\n","            tb200_mean = tb200_validation_correlations.mean()[pred_col]\n","            tb200_std = tb200_validation_correlations.std(ddof=0)[pred_col]\n","            tb200_sharpe = tb200_mean / tb200_std\n","\n","            validation_stats.loc[\"tb200_mean\", pred_col] = tb200_mean\n","            validation_stats.loc[\"tb200_std\", pred_col] = tb200_std\n","            validation_stats.loc[\"tb200_sharpe\", pred_col] = tb200_sharpe\n","\n","        # MMC over validation\n","        mmc_scores = []\n","        corr_scores = []\n","        for _, x in validation_data.groupby(ERA_COL):\n","            series = neutralize_series(unif(x[pred_col]), (x[example_col]))\n","            mmc_scores.append(np.cov(series, x[target_col])[0, 1] / (0.29**2))\n","            corr_scores.append(unif(x[pred_col]).corr(x[target_col]))\n","\n","        val_mmc_mean = np.mean(mmc_scores)\n","        val_mmc_std = np.std(mmc_scores)\n","        corr_plus_mmcs = [c + m for c, m in zip(corr_scores, mmc_scores)]\n","        corr_plus_mmc_sharpe = np.mean(corr_plus_mmcs) / np.std(corr_plus_mmcs)\n","\n","        validation_stats.loc[\"mmc_mean\", pred_col] = val_mmc_mean\n","        validation_stats.loc[\"corr_plus_mmc_sharpe\", pred_col] = corr_plus_mmc_sharpe\n","\n","        # Check correlation with example predictions\n","        per_era_corrs = validation_data.groupby(ERA_COL).apply(\n","            lambda d: unif(d[pred_col]).corr(unif(d[example_col]))\n","        )\n","        corr_with_example_preds = per_era_corrs.mean()\n","        validation_stats.loc[\n","            \"corr_with_example_preds\", pred_col\n","        ] = corr_with_example_preds\n","\n","        # Check exposure dissimilarity per era\n","        tdf = validation_data.groupby(ERA_COL).apply(\n","            lambda df: exposure_dissimilarity_per_era(\n","                df, pred_col, example_col, feature_cols\n","            )\n","        )\n","        validation_stats.loc[\"exposure_dissimilarity_mean\", pred_col] = tdf.mean()\n","\n","    # .transpose so that stats are columns and the model_name is the row\n","    return validation_stats.transpose()"],"metadata":{"id":"7Rtz8hjjpgP4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"id":"O8N-WAmxs2qX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"id":"Pz-q5UgJs4V_"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HEg-KrxVs981"},"source":["class PandasDriver:\n","    def __init__(self, pq_path: str, splits=4):\n","        self.pq_path = pq_path\n","        self.splits = splits\n","        \n","        self.df = pd.read_parquet(pq_path)\n","        self.df['era'] = self.df['era'].astype('int')\n","    \n","    def get_by_group(self, group_id: int, cols=None):\n","        if group_id == self.splits:\n","            group_id = 0\n","        return self.df[self.df['era'] % self.splits == group_id]\n","\n","\n","import duckdb\n","\n","class DuckDBDriver:\n","    def __init__(self, pq_path: str, splits=4):\n","        self.pq_path = pq_path\n","        self.splits = splits\n","        self.conn = duckdb.connect(\":memory:\")\n","        \n","    def _gen_select_statement(self) -> str:\n","        return f\"SELECT * FROM parquet_scan('{self.pq_path}') \"\n","    \n","    def _query(self, expression: str):\n","        return self. conn.execute(expression)\n","    \n","    def _fetch(self, ret_query, fetch_type, cols):\n","        if fetch_type == \"pandas\":\n","            return ret_query.fetchdf()\n","        elif fetch_type == \"numpy\":\n","            return ret_query.fetchdf()[cols].values\n","        \n","    def get_by_era(self, era: str, cols=None, fetch_type=\"pandas\"):\n","        expression = self._gen_select_statement()\n","        expression += f\"WHERE era = '{era}'\"\n","        ret_query = self._query(expression)\n","        return self._fetch(ret_query, fetch_type, cols)\n","    \n","    def get_by_group(self, group_id: int, cols=None, fetch_type=\"pandas\"):\n","        if group_id == self.splits:\n","            group_id = 0\n","        expression = self._gen_select_statement()\n","        expression += f\"WHERE CAST(era AS INT) % {self.splits} = {group_id}\"\n","        ret_query = self._query(expression)\n","        return self._fetch(ret_query, fetch_type, cols)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###########################################################################################\n","################################   CY0xZ_01  ##############################################\n","###########################################################################################"],"metadata":{"id":"9xvh5DVOSfRY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc"],"metadata":{"id":"w3NL5TGdBAqi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import xgboost as xgb\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras import optimizers\n","from sklearn.ensemble import VotingRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error\n","\n","\n","data = pd.read_csv('numerai_training_data.csv')\n","\n","train_data, valid_data = train_test_split(data, test_size=0.2, random_state=42)\n","\n","scaler = StandardScaler()\n","X_train = train_data.iloc[:, 3:-1]\n","X_train = scaler.fit_transform(X_train)\n","y_train = train_data.iloc[:, -1]\n","\n","X_valid = valid_data.iloc[:, 3:-1]\n","X_valid = scaler.transform(X_valid)\n","y_valid = valid_data.iloc[:, -1]\n","\n","inputs = layers.Input(shape=(X_train.shape[1],))\n","x = layers.Dense(64, activation='relu')(inputs)\n","x = layers.Dropout(0.3)(x)\n","x = layers.Dense(32, activation='relu')(x)\n","x = layers.Dropout(0.3)(x)\n","outputs = layers.Dense(1)(x)\n","\n","model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","opt = optimizers.Adam(learning_rate=0.001)\n","model.compile(optimizer=opt, loss='mse')\n","\n","model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_valid, y_valid), verbose=2)\n","\n","params = {\n","    'n_estimators': 1000,\n","    'max_depth': 5,\n","    'learning_rate': 0.01,\n","    'subsample': 0.8,\n","    'colsample_bytree': 0.8,\n","    'objective': 'reg:squarederror',\n","    'n_jobs': -1,\n","    'random_state': 42\n","}\n","\n","xgb_model = xgb.XGBRegressor(**params)\n","xgb_model.fit(X_train, y_train)\n","\n","ensemble = VotingRegressor([\n","    ('nn', model),\n","    ('xgb', xgb_model)\n","])\n","\n","ensemble.fit(X_train, y_train)\n","\n","y_pred = ensemble.predict(X_valid)\n","mse = mean_squared_error(y_valid, y_pred)\n","print('MSE:', mse)\n"],"metadata":{"id":"SzgrsqSUuEul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"pBTJpd0xuWxL"},"execution_count":null,"outputs":[]}]}