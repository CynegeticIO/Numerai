{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model_4.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO4CPduGcLyXjU6YNHBgMz5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"QaJ59yARmd2i"},"source":["import concurrent.futures as _futures\n","import gc\n","import itertools\n","import json\n","import os\n","import random\n","import time\n","from datetime import datetime, timedelta\n","\n","import matplotlib.pyplot as plt\n","import numerapi\n","import numpy as np\n","import pandas as pd\n","import requests\n","from dateutil.relativedelta import FR, relativedelta\n","from sklearn.ensemble import GradientBoostingRegressor\n","from tqdm.auto import tqdm\n","\n","#https://www.alphavantage.co/premium/\n","\n","key = \"<ALPHAVANTAGE API KEY>\"  # $50 USD, 75 calls per minute subscription will work\n","BASE_URL = \"https://www.alphavantage.co/query\"\n","\n","LOAD_DATA_IN_PARALLEL = True\n","\n","TARGET_NAME = \"target\"\n","PREDICTION_NAME = \"signal\"\n","\n","\n","def get_daily_ts_adj(ticker, output_size=\"full\", data_type=\"csv\", backoff=0) -> pd.DataFrame:\n","    \"\"\"Loads one ticker in csv format\"\"\"\n","    time.sleep(backoff)\n","\n","    # WEEKLY\n","    function = \"TIME_SERIES_WEEKLY_ADJUSTED\"\n","    url = (BASE_URL+ f\"?function={function}&symbol={ticker}&apikey={key}&datatype={data_type}\")\n","\n","    data = pd.read_csv(url)\n","    data[\"ticker\"] = ticker\n","\n","    if \"targeting a higher API call\" in data.loc[0][0]:\n","        # retrying...\n","        return get_daily_ts_adj(\n","            ticker, output_size, data_type,\n","            min(60, backoff + random.choice(range(1, 10))),\n","        )\n","\n","    return data\n","\n","\n","def get_tickers_sequential(tickers) -> pd.DataFrame:\n","    \"\"\"Loads a list of tickers sequentially\"\"\"\n","\n","    dfs = []\n","    for ticker in tqdm(tickers):\n","        response = get_daily_ts_adj(ticker)\n","        response[\"ticker\"] = ticker\n","        dfs.append(response)\n","\n","    return pd.concat(dfs)\n","\n","\n","def get_tickers_parallel(tickers) -> pd.DataFrame:\n","    n = 70  # Setting n<75 for extra safety\n","    chunks = [tickers[i : i + n] for i in range(0, len(tickers), n)]\n","\n","    res = []\n","    pbar = tqdm(total=len(tickers))\n","    with _futures.ThreadPoolExecutor() as executor:\n","        for chunk in chunks:\n","            futures = []\n","            for i, ticker in enumerate(chunk):\n","                futures.append(executor.submit(get_daily_ts_adj, ticker=ticker))\n","            for future in _futures.as_completed(futures):\n","                try:\n","                    response = future.result()\n","                    if len(response) < 5:\n","                        pbar.update(1)\n","                        continue\n","                    else:\n","                        res.append(response)\n","                        pbar.update(1)\n","                except KeyboardInterrupt:\n","                    break\n","                except Exception as e:\n","                    pbar.update(1)\n","                    continue\n","\n","    return pd.concat(res)\n","\n","\n","def load_data(tickers, f_path=\"full_data.csv\", threads=False) -> pd.DataFrame:\n","    if os.path.exists(f_path):\n","        data = pd.read_csv(f_path)\n","        data = data.loc[data.ticker.isin(tickers)]\n","    else:\n","        if threads:\n","            data = get_tickers_parallel(tickers)\n","        else:\n","            data = get_tickers_sequential(tickers)\n","        data.to_csv(f_path)\n","\n","    data.set_index(\"timestamp\", inplace=True)\n","    data.index.rename(\"date\", inplace=True)\n","    data.index = pd.to_datetime(data.index)\n","\n","    return data\n","\n","def generate_featues(full_data: pd.DataFrame) -> pd.DataFrame:\n","    ticker_groups = full_data.groupby(\"bloomberg_ticker\")\n","    indicators = []\n","    sma_periods = [2, 5, 21, 50, 200]\n","    ema_periods = [2, 5, 21, 50, 200]\n","\n","    for period in sma_periods:\n","        full_data[f\"close_SMA_{period}\"] = ticker_groups[\"adjusted close\"].transform(\n","            lambda x: x.rolling(period).mean()\n","        )\n","        indicators.append(f\"close_SMA_{period}\")\n","\n","    for period in ema_periods:\n","        full_data[f\"close_EMA_{period}\"] = ticker_groups[\"adjusted close\"].transform(\n","            lambda x: x.ewm(span=period).mean()\n","        )\n","        indicators.append(f\"close_EMA_{period}\")\n","\n","    full_data.dropna(inplace=True, axis=0)\n","\n","    date_groups = full_data.groupby(full_data.index)\n","    for indicator in tqdm(indicators):\n","        gc.collect()\n","        full_data.loc[:, f\"{indicator}_quintile\"] = (\n","            date_groups[indicator]\n","            .apply(lambda group: pd.qcut(group, 5, labels=False, duplicates=\"drop\"))\n","            .astype(np.float16)\n","        )\n","        gc.collect()\n","    del date_groups\n","    gc.collect()\n","\n","    return full_data\n","\n","\n","def main():\n","    napi = numerapi.SignalsAPI()\n","\n","    # Numerai Universe\n","    eligible_tickers = pd.Series(napi.ticker_universe(), name=\"bloomberg_ticker\")\n","    print(f\"Number of eligible tickers : {len(eligible_tickers)}\")\n","\n","    ticker_map = pd.read_csv(\n","        \"https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_ticker_map_w_bbg.csv\"\n","    )\n","\n","    # ----- Yahoo <-> Bloomberg mapping -----\n","    yfinance_tickers = eligible_tickers.map(\n","        dict(zip(ticker_map[\"bloomberg_ticker\"], ticker_map[\"yahoo\"]))\n","    ).dropna()\n","    bloomberg_tickers = ticker_map[\"bloomberg_ticker\"]\n","    print(f\"Number of eligible, mapped tickers: {len(yfinance_tickers)}\")\n","\n","    us_ticker_map = ticker_map[ticker_map.bloomberg_ticker.str[-2:] == \"US\"]\n","    #tickers = us_ticker_map.yahoo.dropna().values #for US tickers\n","    tickers = ticker_map.yahoo.dropna().values #For possible tickers\n","\n","    # ----- Raw data loading and formatting -----\n","    print(f\"using tickers: {len(tickers)}\")\n","    full_data = load_data(tickers, \"full_data.csv\", threads=LOAD_DATA_IN_PARALLEL)\n","\n","    full_data[\"bloomberg_ticker\"] = full_data.ticker.map(\n","        dict(zip(ticker_map[\"yahoo\"], bloomberg_tickers))\n","    )\n","\n","    full_data = full_data[\n","        [\"bloomberg_ticker\", \"open\", \"high\", \"low\", \"close\", \"adjusted close\"]\n","    ].sort_index(ascending=True)\n","    full_data.dropna(inplace=True, axis=0)\n","\n","    # ----- Merging targets -----\n","    url = \"https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_train_val_bbg.csv\"\n","    targets = pd.read_csv(url)\n","\n","    targets[\"target\"] = targets[\"target\"].astype(np.float16)\n","    targets[\"date\"] = pd.to_datetime(targets[\"friday_date\"], format=\"%Y%m%d\")\n","    gc.collect()\n","\n","    # ----- Generate and select features -----\n","    full_data = generate_featues(full_data)\n","    feature_names = [f for f in full_data.columns if \"quintile\" in f]\n","\n","    ML_data = pd.merge(\n","        full_data.reset_index(), targets, on=[\"date\", \"bloomberg_ticker\"],\n","    ).set_index(\"date\")\n","    print(f\"Number of eras in data: {len(ML_data.index.unique())}\")\n","\n","    ML_data = ML_data[ML_data.index.weekday == 4]\n","    ML_data = ML_data[ML_data.index.value_counts() > 200]\n","\n","    # ----- Train test split -----\n","    train_data = ML_data[ML_data[\"data_type\"] == \"train\"]\n","    test_data = ML_data[ML_data[\"data_type\"] == \"validation\"]\n","\n","    corrs = train_data.groupby(train_data.index).apply(\n","        lambda x: x[feature_names+[TARGET_NAME]].corr()[TARGET_NAME]\n","    )\n","    mean_corr = corrs[feature_names].mean(0)\n","    print(mean_corr)\n","\n","    last_friday = datetime.now() + relativedelta(weekday=FR(-1))\n","    print(last_friday)\n","    date_string = last_friday.strftime(\"%Y-%m-%d\")\n","\n","    try:\n","        live_data = full_data.loc[date_string].copy()\n","    except KeyError as e:\n","        print(f\"No ticker on {e}\")\n","        live_data = full_data.iloc[:0].copy()\n","    live_data.dropna(subset=feature_names, inplace=True)\n","    print(len(live_data))\n","    # ----- Train model -----\n","    print(\"Training model...\")\n","    model = GradientBoostingRegressor()\n","    model.fit(train_data[feature_names], train_data[TARGET_NAME])\n","    print(\"Model trained.\")\n","\n","    # ----- Predict test data -----\n","    train_data[PREDICTION_NAME] = model.predict(train_data[feature_names])\n","    test_data[PREDICTION_NAME] = model.predict(test_data[feature_names])\n","    live_data[PREDICTION_NAME] = model.predict(live_data[feature_names])\n","\n","    diagnostic_df = pd.concat([test_data, live_data])\n","    diagnostic_df[\"friday_date\"] = diagnostic_df.friday_date.fillna(\n","        last_friday.strftime(\"%Y%m%d\")\n","    ).astype(int)\n","    diagnostic_df[\"data_type\"] = diagnostic_df.data_type.fillna(\"live\")\n","    diagnostic_df[\n","        [\"bloomberg_ticker\", \"friday_date\", \"data_type\", \"signal\"]\n","    ].reset_index(drop=True).to_csv(\"example_signal_alphavantage.csv\", index=False)\n","    print(\n","        \"Submission saved to example_signal_alphavantage.csv. Upload to signals.numer.ai for scores and diagnostics\"\n","    )\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":null,"outputs":[]}]}