{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model_3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN2hGa7oXHUpdeXG/tddDse"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"e-xLU850lNP6"},"source":["import datetime\n","import gc\n","import logging\n","import os\n","from datetime import datetime, timedelta\n","\n","import matplotlib.pyplot as plt\n","import numerapi\n","import numpy as np\n","import pandas as pd\n","import quandl\n","import requests\n","from dateutil.relativedelta import FR, relativedelta\n","from sklearn.ensemble import GradientBoostingRegressor\n","\n","logging.basicConfig()\n","\n","quandl_log = logging.getLogger(\"quandl\")\n","quandl_log.setLevel(logging.DEBUG)\n","\n","API_KEY = \"<Quandl API KEY>\"\n","quandl.ApiConfig.api_key = API_KEY\n","\n","# -----Helper functions for feature extraction-----\n","def RSI(prices, interval=14):\n","    \"\"\"Computes Relative Strength Index given a price series and lookback interval\n","    Modified from https://stackoverflow.com/questions/20526414/relative-strength-index-in-python-pandas\n","    See more here https://www.investopedia.com/terms/r/rsi.asp\"\"\"\n","    delta = prices.diff()\n","\n","    dUp, dDown = delta.copy(), delta.copy()\n","    dUp[dUp < 0] = 0\n","    dDown[dDown > 0] = 0\n","\n","    RolUp = dUp.rolling(interval).mean()\n","    RolDown = dDown.rolling(interval).mean().abs()\n","\n","    RS = RolUp / RolDown\n","    RSI = 100.0 - (100.0 / (1.0 + RS))\n","    return RSI\n","\n","\n","# -----Data loading function-----\n","def download_full_and_load(ticker_map, common_tickers, f_name: str = \"full_EOD.zip\") -> pd.DataFrame:\n","    \"\"\"Downloads a zip of entire dataset and load csv from it.\n","    Much faster!\n","    \"\"\"\n","\n","    url = f\"https://www.quandl.com/api/v3/databases/EOD/data?api_key={API_KEY}\"\n","\n","    if os.path.exists(f_name):\n","        print(f\"Using downloaded file {f_name}\")\n","    else:\n","        print(\"Downloading data...\")\n","        with requests.get(url, stream=True) as r:\n","            with open(f_name, \"wb\") as fin_data:\n","                for chunk in r.iter_content(chunk_size=1024):\n","                    fin_data.write(chunk)\n","            print(f\"Saved as: {f_name}\")\n","\n","    # column names in the csv file without headers\n","    cols = [\n","        \"ticker\", \"date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Dividend\",\n","        \"Split\", \"Adj_Open\", \"Adj_High\", \"Adj_Low\", \"Adj_Close\", \"Adj_Volume\",\n","    ]\n","\n","    # usecols refers to the column in the csv.\n","    # using only [ticker, date, adj_open, adj_close]\n","    # Loading only needed columns as FP32\n","    print(\"loading from csv...\")\n","    full_data = pd.read_csv(\n","        f_name,\n","        usecols=[0, 1, 9, 12],\n","        compression=\"zip\",\n","        dtype={0: str, 1: str, 9: np.float32, 12: np.float32},\n","        header=None,\n","    )\n","\n","    # renaming the columns\n","    filter_columns = [\"ticker\", \"date\", \"Adj_Open\", \"Adj_Close\"]\n","    full_data.columns = filter_columns\n","    full_data.set_index(\"date\", inplace=True)\n","    full_data.index = pd.to_datetime(full_data.index)\n","\n","    full_data.rename(\n","        columns={\n","            \"Adj_Open\": \"open\",\n","            \"Adj_Close\": \"close\",\n","        },\n","        inplace=True,\n","    )\n","\n","    full_data[[\"open\", \"close\"]] = full_data[[\"open\", \"close\"]].astype(np.float32)\n","    full_data = full_data[full_data.ticker.isin(common_tickers)]\n","    full_data[\"bloomberg_ticker\"] = full_data.ticker.map(\n","        dict(zip(ticker_map[\"yahoo\"], ticker_map[\"bloomberg_ticker\"]))\n","    )\n","    full_data.sort_index(ascending=True, inplace=True)\n","    gc.collect()\n","\n","    return full_data\n","\n","\n","def main():\n","    # -----Tickers and mapping-----\n","    napi = numerapi.SignalsAPI()\n","    eligible_tickers = pd.Series(napi.ticker_universe(), name=\"bloomberg_ticker\")\n","\n","    ticker_map = pd.read_csv(\n","        \"https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_ticker_map_w_bbg.csv\"\n","    )\n","    ticker_map = ticker_map[ticker_map.bloomberg_ticker.isin(eligible_tickers)]\n","\n","    numerai_tickers = ticker_map[\"bloomberg_ticker\"]\n","    yfinance_tickers = ticker_map[\"yahoo\"]\n","\n","    eod_tickers = pd.read_csv(\n","        \"https://s3.amazonaws.com/quandl-production-static/end_of_day_us_stocks/ticker_list.csv\"\n","    )\n","    print(f\"Number of eligible tickers : {len(eligible_tickers)}\")\n","\n","    common_tickers = np.intersect1d(\n","        yfinance_tickers.values.astype(str), eod_tickers[\"Ticker\"].values.astype(str)\n","    )\n","    print(f\"Number of tickers common between EOD and Bloomberg: {len(common_tickers)}\")\n","\n","    # downloads the whole dataset as zip and read data (takes around 1.5min)\n","    full_data = download_full_and_load(ticker_map, common_tickers, f_name=\"full_EOD.zip\")\n","\n","    # Building a custom feature\n","    full_data[\"day_chg\"] = full_data[\"close\"] / full_data[\"open\"] - 1\n","    gc.collect()\n","\n","    # -----Feature engineering-----\n","    ticker_groups = full_data.groupby(\"bloomberg_ticker\")\n","\n","    # RSI\n","    full_data[\"close_RSI_14\"] = ticker_groups[\"close\"].transform(lambda x: RSI(x, 14))\n","    full_data[\"close_RSI_21\"] = ticker_groups[\"close\"].transform(lambda x: RSI(x, 21))\n","    full_data[\"day_chg_RSI_14\"] = ticker_groups[\"day_chg\"].transform(\n","        lambda x: RSI(x, 14)\n","    )\n","    full_data[\"day_chg_RSI_21\"] = ticker_groups[\"day_chg\"].transform(\n","        lambda x: RSI(x, 21)\n","    )\n","\n","    # SMA\n","    full_data[\"close_SMA_14\"] = ticker_groups[\"close\"].transform(\n","        lambda x: x.rolling(14).mean()\n","    )\n","    full_data[\"close_SMA_21\"] = ticker_groups[\"close\"].transform(\n","        lambda x: x.rolling(21).mean()\n","    )\n","\n","    indicators = [\"close_RSI_14\", \"close_RSI_21\", \"day_chg_RSI_14\",\n","                  \"close_SMA_14\", \"close_SMA_21\", \"day_chg_RSI_21\"]\n","\n","    full_data.dropna(axis=0, inplace=True)\n","    del ticker_groups\n","\n","    # -----Feature engineering: Quintile-----\n","    date_groups = full_data.groupby(full_data.index)\n","    print(\"Quintiling...\")\n","    for indicator in indicators:\n","        full_data[f\"{indicator}_quintile\"] = (\n","            date_groups[indicator]\n","            .transform(\n","                lambda group: pd.qcut(group, 100, labels=False, duplicates=\"drop\")\n","            )\n","            .astype(np.float16)\n","        )\n","        gc.collect()\n","\n","    del date_groups\n","    gc.collect()\n","\n","    # -----Feature engineering: Quintile lag-----\n","    ticker_groups = full_data.groupby(\"ticker\")\n","    # create lagged features, lag 0 is that day's value, lag 1 is yesterday's value, etc\n","    print(\"Calculating lag...\")\n","    for indicator in indicators:\n","        num_days = 5\n","        for day in range(num_days + 1):\n","            full_data[f\"{indicator}_quintile_lag_{day}\"] = ticker_groups[\n","                f\"{indicator}_quintile\"\n","            ].transform(lambda group: group.shift(day))\n","\n","        gc.collect()\n","\n","    full_data.dropna(axis=0, inplace=True)\n","\n","    del ticker_groups\n","    gc.collect()\n","    print(\"Calculating changes in lag...\")\n","    # create difference of the lagged features (change in RSI quintile by day)\n","    for indicator in indicators:\n","        for day in range(0, num_days):\n","            full_data[f\"{indicator}_diff_{day}\"] = (\n","                full_data[f\"{indicator}_quintile_lag_{day}\"]\n","                - full_data[f\"{indicator}_quintile_lag_{day + 1}\"]\n","            ).astype(np.float16)\n","            gc.collect()\n","\n","    # create difference of the lagged features (change in RSI quintile by day)\n","    for indicator in indicators:\n","        full_data[f\"{indicator}_abs_diff_{day}\"] = np.abs(\n","            full_data[f\"{indicator}_quintile_lag_{day}\"]\n","            - full_data[f\"{indicator}_quintile_lag_{day + 1}\"]\n","        ).astype(np.float16)\n","        gc.collect()\n","\n","    TARGET_NAME = \"target\"\n","    PREDICTION_NAME = \"signal\"\n","\n","    # read in Signals targets\n","    numerai_targets = \"https://numerai-signals-public-data.s3-us-west-2.amazonaws.com/signals_train_val_bbg.csv\"\n","    targets = pd.read_csv(numerai_targets)\n","    targets[\"date\"] = pd.to_datetime(targets[\"friday_date\"], format=\"%Y%m%d\")\n","\n","    # merge our feature data with Numerai targets\n","    ML_data = pd.merge(\n","        full_data.reset_index(), targets, on=[\"date\", \"bloomberg_ticker\"]\n","    ).set_index(\"date\")\n","    print(f\"Number of eras in data: {len(ML_data.index.unique())}\")\n","\n","    # for training and testing we want clean, complete data only\n","    ML_data.dropna(inplace=True)\n","    ML_data = ML_data[ML_data.index.weekday == 4]  # ensure we have only fridays\n","    ML_data = ML_data[\n","        ML_data.index.value_counts() > 200\n","    ]  # drop eras with under 200 observations per era\n","    feature_names = [f for f in ML_data.columns for y in [\"lag\", \"diff\"] if y in f]\n","    print(f\"Using {len(feature_names)} features\")\n","\n","    last_friday = datetime.now() + relativedelta(weekday=FR(-1))\n","    date_string = last_friday.strftime(\"%Y-%m-%d\")\n","\n","    try:\n","        live_data = full_data.loc[date_string].copy()\n","    except KeyError as e:\n","        print(f\"No ticker on {e}\")\n","        live_data = full_data.iloc[:0].copy()\n","    live_data.dropna(subset=feature_names, inplace=True)\n","\n","    # get data from the day before, for markets that were closed\n","    # on the most recent friday\n","    last_thursday = last_friday - timedelta(days=1)\n","    thursday_date_string = last_thursday.strftime(\"%Y-%m-%d\")\n","    thursday_data = full_data.loc[thursday_date_string]\n","    # Only select tickers than aren't already present in live_data\n","    thursday_data = thursday_data[\n","        ~thursday_data.ticker.isin(live_data.ticker.values)\n","    ].copy()\n","    thursday_data.dropna(subset=feature_names, inplace=True)\n","\n","    live_data = pd.concat([live_data, thursday_data])\n","\n","    # train test split\n","    train_data = ML_data[ML_data[\"data_type\"] == \"train\"].copy()\n","    test_data = ML_data[ML_data[\"data_type\"] == \"validation\"].copy()\n","\n","    train_data[feature_names] /= 100.0\n","    test_data[feature_names] /= 100.0\n","    live_data[feature_names] /= 100.0\n","\n","    del ML_data\n","    gc.collect()\n","\n","    # train model\n","    print(\"Training model...\")\n","    model = GradientBoostingRegressor(n_estimators=50)\n","    model.fit(train_data[feature_names], train_data[TARGET_NAME])\n","    print(\"Model trained.\")\n","\n","    # predict test data\n","    train_data[PREDICTION_NAME] = model.predict(train_data[feature_names])\n","    test_data[PREDICTION_NAME] = model.predict(test_data[feature_names])\n","\n","    print(f\"Number of live tickers to submit: {len(live_data)}\")\n","    live_data[PREDICTION_NAME] = model.predict(live_data[feature_names])\n","\n","    # prepare and writeout example file\n","    diagnostic_df = pd.concat([test_data, live_data])\n","    diagnostic_df[\"friday_date\"] = diagnostic_df.friday_date.fillna(\n","        last_friday.strftime(\"%Y%m%d\")\n","    ).astype(int)\n","    diagnostic_df[\"data_type\"] = diagnostic_df.data_type.fillna(\"live\")\n","    diagnostic_df[\n","        [\"bloomberg_ticker\", \"friday_date\", \"data_type\", \"signal\"]\n","    ].reset_index(drop=True).to_csv(\"example_quandl_signal_upload.csv\", index=False)\n","    print(\n","        \"Example submission completed. Upload to signals.numer.ai for scores and live submission\"\n","    )\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":null,"outputs":[]}]}